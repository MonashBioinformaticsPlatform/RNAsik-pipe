{
    "docs": [
        {
            "location": "/", 
            "text": "RNAsik for RNAseq\n\n\n\n\nN.B\n This workflow assumes model organism has a reference genome. If the reference genome isn't applicable, different workflow is required.\n\n\n\n\nAbout\n\n\nRNAsik\n pipeline was build in house for processing \nRNA-seq(uencing)\n data.\nIt is written in \nBigDataScript (bds)\n, which is \ndomain specific language (DSL)\n, that makes writing pipelines easy as well as making them robust. To get a bit more technical, bds runs on \njava virtual machine (JVM)\n and therefore requires \nJava\n.\n\n\nIn simple terms any pipeline is a wrapper of several tools that makes it easier and arguably faster to get to the end goal. The three core parts to any \nRNA-seq analysis\n are: \n\n\n\n\nmapping to the reference genome\n\n\ncounting reads mapped into features e.g genes\n\n\ndoing differential expression (DE) statistics\n\n\n\n\nThe pipeline does the first two parts and \nDegust\n does the third part. Degust itself, in simple terms, a wrapper around \nlimma\n and \nedgeR\n R packages. In theory and practice one can take output from \nRNAsik\n pipeline, which is a table of counts where every gene is a row and every column is a sample and use those with any other R packages that do DE analysis.\n\n\nIn actual terms both \nRNAsik\n and \nDegust\n provide complete experience, not only you'll get your list of DE genes and QC metrics, but will be able to get full inside into your experimental design and the outcome of that. \nRNAsik\n does read alignment and read counting and cleaning and improvements of your table of counts, which makes \nDegust\n analysis one upload away. \nRNAsik\n wraps \nthese tools\n making your RNAseq analysis more streamline. It also has \"sanity checks\" inbuilt, checking command line options, checking if options are valid files/directories and it will talk to you so don't sweat :) but do read the error messages. \nDegust\n is exceptionally good for exploratory data visualisation and analysis. Both tools can also server as a nice proxy for learning bioinformatics as they provide command line and R code for doing the analysis. Last but not least thanks to \nMultiQC\n \nRNAsik\n provides an aggregate of different metrics in one place - multiqc report. This is a good place to start understanding your data.\n\n\nThe central bits of information are:\n\n\n\n\nAre there differences in library sizes?\n\n\nIs there any issues with mapping rates?\n\n\nIs there any issues with reads assignment rates?\n\n\n\n\nHowever there is so many other questions you can ask including:\n\n\n\n\nWhat is duplication rate?\n\n\nWhat is multi-mapping rate?\n\n\nWhat is intragenic and interagenic rates?\n\n\n\n\nAs mentioned above \nmultiqc\n report is a great first step in the attempt to answer those questions. A lot of the time everything looks fairly good and consistent allowing downstream analysis. Sometimes user can tweak certain individual parameters which can improve results, other times it comes down to experimental design and/or library preparation and sequencing issues. Either way one need to make this \"first iteration\" in order to see room for improvement. \n\n\nHow to cite\n\n\nAt the moment the only way to cite is point to github repository. I'm in the process of obtaining \ndoi\n reference.\n\n\n\n\n\n\nTsyganov, K. Perry, A. Archer, S. Powell, D. (2018, January 25). \nRNAsik pipeline for RNA sequencing analysis\n. https://github.com/MonashBioinformaticsPlatform/RNAsik-pipe\n\n\n\n\n\n\nIt can be hard to give full acknowlegment to all contributors. The nature of the open source projects such that contributors can come and go, however they leave behind valuable contributions and need to get full credit for that. Please look at \nRNAsik GitHub\n repository to get a full sense of who is contributing. In particular one can look at \nnumber of commits\n, \nissues triaging and handling\n and \npull requests (PRs)\n. Please also remember that every contribution matters, nothing is too small!\n\n\nMBP team photo\n\n\n\n\nTweet to @kizza_a\n \n\n\n\n\n\n\nShare", 
            "title": "About"
        }, 
        {
            "location": "/#rnasik-for-rnaseq", 
            "text": "N.B  This workflow assumes model organism has a reference genome. If the reference genome isn't applicable, different workflow is required.", 
            "title": "RNAsik for RNAseq"
        }, 
        {
            "location": "/#about", 
            "text": "RNAsik  pipeline was build in house for processing  RNA-seq(uencing)  data.\nIt is written in  BigDataScript (bds) , which is  domain specific language (DSL) , that makes writing pipelines easy as well as making them robust. To get a bit more technical, bds runs on  java virtual machine (JVM)  and therefore requires  Java .  In simple terms any pipeline is a wrapper of several tools that makes it easier and arguably faster to get to the end goal. The three core parts to any  RNA-seq analysis  are:    mapping to the reference genome  counting reads mapped into features e.g genes  doing differential expression (DE) statistics   The pipeline does the first two parts and  Degust  does the third part. Degust itself, in simple terms, a wrapper around  limma  and  edgeR  R packages. In theory and practice one can take output from  RNAsik  pipeline, which is a table of counts where every gene is a row and every column is a sample and use those with any other R packages that do DE analysis.  In actual terms both  RNAsik  and  Degust  provide complete experience, not only you'll get your list of DE genes and QC metrics, but will be able to get full inside into your experimental design and the outcome of that.  RNAsik  does read alignment and read counting and cleaning and improvements of your table of counts, which makes  Degust  analysis one upload away.  RNAsik  wraps  these tools  making your RNAseq analysis more streamline. It also has \"sanity checks\" inbuilt, checking command line options, checking if options are valid files/directories and it will talk to you so don't sweat :) but do read the error messages.  Degust  is exceptionally good for exploratory data visualisation and analysis. Both tools can also server as a nice proxy for learning bioinformatics as they provide command line and R code for doing the analysis. Last but not least thanks to  MultiQC   RNAsik  provides an aggregate of different metrics in one place - multiqc report. This is a good place to start understanding your data.  The central bits of information are:   Are there differences in library sizes?  Is there any issues with mapping rates?  Is there any issues with reads assignment rates?   However there is so many other questions you can ask including:   What is duplication rate?  What is multi-mapping rate?  What is intragenic and interagenic rates?   As mentioned above  multiqc  report is a great first step in the attempt to answer those questions. A lot of the time everything looks fairly good and consistent allowing downstream analysis. Sometimes user can tweak certain individual parameters which can improve results, other times it comes down to experimental design and/or library preparation and sequencing issues. Either way one need to make this \"first iteration\" in order to see room for improvement.", 
            "title": "About"
        }, 
        {
            "location": "/#how-to-cite", 
            "text": "At the moment the only way to cite is point to github repository. I'm in the process of obtaining  doi  reference.    Tsyganov, K. Perry, A. Archer, S. Powell, D. (2018, January 25).  RNAsik pipeline for RNA sequencing analysis . https://github.com/MonashBioinformaticsPlatform/RNAsik-pipe    It can be hard to give full acknowlegment to all contributors. The nature of the open source projects such that contributors can come and go, however they leave behind valuable contributions and need to get full credit for that. Please look at  RNAsik GitHub  repository to get a full sense of who is contributing. In particular one can look at  number of commits ,  issues triaging and handling  and  pull requests (PRs) . Please also remember that every contribution matters, nothing is too small!", 
            "title": "How to cite"
        }, 
        {
            "location": "/#mbp-team-photo", 
            "text": "Tweet to @kizza_a     \nShare", 
            "title": "MBP team photo"
        }, 
        {
            "location": "/docs/", 
            "text": "Documentation\n\n\nQuick start\n\n\nInstall\n\n\nconda config --add channels defaults\nconda config --add channels conda-forge\nconda config --add channels bioconda\nconda install -c serine rnasik \nconda install -c bioconda qualimap \n\n\n\n\nAlign raw reads\n\n\nRNAsik -align star \\\n       -fastaRef /path/to/reference.fasta \\\n       -fqDir /path/to/raw-data/directory\n\n\n\n\nCount gene features\n\n\nRNAsik -counts \\\n       -gtfFile path/to/annotation.gtf\n\n\n\n\nThe lot\n\n\nRNAsik -fqDir /path/to/raw-data/directory \\\n       -align star \\\n       -refFiles /path/to/refDir \\\n       -counts \\\n       -all\n\n\n\n\nData set for testing\n\n\n\n\nN.B \nRNAsik\n pipeline is some what resource hungry. This isn't \nRNAsik\n fault per say, because it \"simply\" wraps other tools. STAR aligner required fair amount of RAM and cpus. For a large genome like mouse it required around 30 Gb of RAM and the more cpus you have the quicker you'll map. I would advise not run the pipeline with less than 4 cores, which is default. This testing data set is of yeast and requires about 14 Gb of RAM. Also read \nthis comment\n, in summary you might have \nRNAsik\n failing on system that are under minimum system resources, this is work in progess and should be fixed in the future.\n\n\n\n\nI figured that for testing you need smallish data set as well as species with a small genome, as indexing of genome takes a while for larger genome e.g mouse\nI found this study \nGSE103004\n which looks like an open access. If you follow \nthat link\n you should hit front GEO page for that study. You can find your way to actual data (SRA files) files, but I always find it's a bit convoluted, so hit \nhere is a link to data files\n. \n\n\nI've already prepared raw-data (fastq) files for you. I also reduced number of samples and sub-sampled reads to speed up your test run. Firstly though let me explain to you how to get full data set.\n\n\n\n\n\n\ndownload sratoolkit\n which is set of tools from \nNCBI\n that you'll need to download \nsra files\n and then extract/convert those to fastq files.\n\n\n\n\n\n\nfastq-dump --gzip --split-files SRR3407195\n this is a command that you'll want to run to get one particular sra file, not \n--split-files\n options, you need to use that if you data is paired end. If you don't use that flag, then you are going to end up with a single fastq file that has reads interleaved or truncated/merged in a funny way (had some issues like that in the past)\n\n\n\n\n\n\nHowever you don't want run that command several times, so use a loop\n\n\nwhile read s; do fastq-dump --gzip --split-files $s \n $s.log 2\n1 \ndone \n SRR_Acc_List.txt\n\n\n\n\nYou can download \nSRR_Acc_List.txt file at this page\n (mentioned that page before). That list has 9 sra files corresponding to 9 samples, where each samples was paired end and therefore total number of files is double - 18. \n\n\nAlso note that default marking when extracting from sra for R1 and R2 is \n_1\n and \n_2\n respectively and so if you are running \nRNAsik\n on that full data set you'll need to pass \n-pairIds \"_1,_2\"\n flag, default is \n-pairIds \"_R1,_R2\"\n \n\n\nIf you want nicely labeled bam and then counts you can pass \n-samplesSheet samplesSheet.txt\n. I haven't implemented url based samples sheets, so you'll need to download one before hand from \nhere\n. I'll include handling of url based samples sheets into roadmap, so watch that space !\n\n\nIf you ran \nRNAsik\n on a full data set and then used \nDegust\n for DGE analysis you should get \nthese results\n.\n\n\nNote that report was generated using \npandoc\n with custome template and \nigv_links\n were created using \nmk_igv_links\n script located in \nscripts/\n directory\n\n\nTry it out\n\n\nRNAsik -align star \\\n       -fastaRef ftp://ftp.ensembl.org/pub/release-91/fasta/saccharomyces_cerevisiae/dna/Saccharomyces_cerevisiae.R64-1-1.dna_sm.toplevel.fa.gz \\\n       -fqDir http://bioinformatics.erc.monash.edu/home/kirill/sikTestData/rawData/IndustrialAntifoamAgentsYeastRNAseqData.tar \\\n       -counts \\\n       -gtfFile ftp://ftp.ensembl.org/pub/release-91/gtf/saccharomyces_cerevisiae/Saccharomyces_cerevisiae.R64-1-1.91.gtf.gz \\\n       -all \\\n       -paired\n\n\n\n\nIntroduction\n\n\nAs mentioned previously in \nabout section\n very first step in \nRNA-seq analysis\n is to map your raw reads (\nFASTQ\n) to the reference genome following by counting of reads that map onto a feature. But there is always more you could do with your data, in fact almost always only by doing more you can get deeper inside into your biological experiment and the system you are studying. And so \nRNAsik\n uses these tools to get as much out of your data as possible in an streamline run:\n\n\n\n\nSTAR aligner for mapping\n\n\nfeatureCounts from subread package for read counting\n\n\nsamtools for coverage calculation and general bam files filtering\n\n\npicard tools also for general bam fiels filtering\n\n\nQualiMap for intragenic and interegenic rates\n\n\nFastQC for QC metrics on yor fastq files\n\n\nMultiQC for wraping everying into nice, single page report\n \n\n\n\n\nAs one can imagine every one of those tools has several number of options and by running \nRNAsik-pipeline\n you get predefined - subjective run. Obviously it all comes from years of experience and continues development and improvement. Use can always pass his/her own options through \n-extraOptions\n flag for more fine turning. \nAlternatively as, hinted above, user can leverage of \nRNAsik\n to run everything separately with fine control over the individual run. \nRNAsik\n produces \n.html report\n with all commands options specified.\n\n\nInstallation\n\n\nUsing conda\n\n\n\n\ndownload \nminiconda\n \n.sh\n installer \n\n\nrun it and follow the prompts\n\n\n\n\nbash Miniconda3-latest-Linux-x86_64.sh \n\n\n\n\n\n\nadd a few \nconda\n \"channels\", this is so \nconda\n knows where to get things from\n\n\n\n\nconda config --add channels defaults\nconda config --add channels conda-forge\nconda config --add channels bioconda\n\n\n\n\n\n\ninstall \nRNAsik\n pipeline\n\n\n\n\nconda install -c serine rnasik \n\n\n\n\n\n\nadditionally install \nqualimap\n separately (it was a tricky to include qualimap into RNAsik because large number of dependencies that qualimap has)\n\n\n\n\nconda install -c bioconda qualimap \n\n\n\n\n\nRight now \nRNAsik\n hosted from my \"channel\" (conda terminology). There are plans to push it to official \nbioconda channel\n\n\nconda extras\n\n\n\n\nsearch \"main\" label (repository)\n\n\n\n\nconda search -c serine rnasik\n\n\n\n\n\n\nsearch \"dev\" label (repository)\n\n\n\n\nconda search -c serine/label/dev rnasik\n\n\n\n\n\n\ninstall specific version\n\n\n\n\nconda install -c serine/label/dev \nrnasik=1.5.1+3ecd215\n\n\n\n\n\n\n\ninstall specific version and specific build number\n\n\n\n\nconda install -c serine/label/dev \nrnasik=1.5.1+3ecd215=4\n\n\n\n\n\n\nsimply install latest from that \"dev\" label (repository)\n\n\n\n\nconda install -c serine/label/dev rnasik\n\n\n\n\nNOTE:\n that these are just some extras commands mainly for testing purposes not for production use !\n\n\nAlternative installation methods\n\n\n\n\nHERE\n\n\n\n\nUser input\n\n\nReference files\n\n\n\n\nInput File\nDescription\n\n\nFASTA file\nMost often this is your genomic reference sequence. It is a FASTA file holding raw DNA sequences where different features e.g chromosomes are labeled uniquely with a header line starting with '>'. [FASTA Format Description](https://en.wikipedia.org/wiki/FASTA_format) \n\n\nGTF/GFF/SAF file\n This is your gene annotation file (i.e coordinates of your genes, exons and other genomic features). This should be linked and associated with your genomic reference file. SAF (simple annotation format) is something that featureCounts use and it supported by the pipeline\n\n\n\n\n\nRaw data\n\n\n\n\nInput File\nDescription\n\n\nFASTQ file\nThese are your raw files that are provided by the sequencing facility to you, they can be gzipped (.fq, .fastq, .fq.gz, .fastq.gz) \n\n\n\n\n\nUser input explained\n\n\nAnnotation files\n\n\nAnnotation file would central for differential expression (DE) analysis without one you won't be able to do one. You could have very well assembled genome with very good mapping rate, but unless you know where your genes are on that genome i.e start and end coordinates for your features e.g genes you won't be able to deduce any information about those features and therefore compare between conditions. Below is an example of bear minimum information you need for feature counting. \n\n\nGeneID  Chr Start   End Strand\n497097  chr1    3204563 3207049 -\n497097  chr1    3411783 3411982 -\n497097  chr1    3660633 3661579 -\n\n\n\n\nThere few entities that provide genome annotation, some cover more species than other. There will be of course individuals that simply provide annotation for one particular species, perhaps for more rare model organisms.\n\n\nThere are also different annotation file formats out there, which makes a little hard to provide \nRNAsik\n support for all of them. Currently \nRNAsik\n can only work with \nGFF\n, \nGTF\n or \nSAF\n file formats. There are many compatibilities issues between formats, but more importantly certain bits of information are only found in some of the files. The example above show \nSAF\n file format and as you can see that includes not human redable gene names nor biotype. \nGFF\n also often doesn't have biotype information, but on the other hand has product tag, which has short description, for protein coding at least, of resulting protein product, \nGTF\n lacks that information. Because of all these little nuances it can be hard to capture all of the desirable information.\n\n\nMost tools in the pipeline prefer \nGTF\n, some can only work with \nGTF\n. I guess main reason for this is that every line is self contained and the format has been fairly predictable/stable.\n\n\nIf for whatever reason you can't get hold of \nGFF/GTF\n files and your annotation comes in \nGenBank\n (very common for bacterial genomes) or \nBed\n files, don't panic and try to parse those files into \nSAF\n format. There are plans to include \ngb_parse.py\n script that should help most people with \nGenBank\n files.\n\n\nIrrespective of which reference file distributor and which annotation file you are going to use, it is highly recommended that both of those files come from the same distributor. Most common distributors are \nEnsembl\n, \nUCSC\n and \nNCBI\n.\n\n\nRaw data files\n\n\nRaw data is something that you should take good care of. You can regenerate all other data files, but you can't really regenerate you raw data, not unless you have lots of money and time. So be sure to back your \nfastq\n files up and never mess/do (i.e modify) your original fastq files. If you want to try something out, make a copy and do whatever you are doing on a copy. Also there will never be a need to unzip your fastq file. All of you fastq file should be gziped and have file extension \n.fastq.gz\n or \nfq.gz\n or something similar. \n\n\nRNAsik\n will search recursively your \n-fqDir\n and find all fastq files. \nRNAsik\n can handle nested directories as long as your data is homogeneous i.e all data belongs to the same library type single-end or paired-end. If data is paired end, \nRNAsik\n uses \n-pairIds\n value to figure out read pairs. You can check that all of your fastq files had been found by looking into \nsikRun/logs/samples/fqFiles\n. \n\n\nAfter obtaining a list of all fastq files \nRNAsik\n tries to be smart and attempts to group fastq files into samples, that is R1 and R2 reads are grouped, but also any fastq files that had been split across lanes should also be grouped. You should end up, after the run, with the same number of bam files as you have samples. Again you can check grouping in \nsikRun/logs/samples/fqMap\n\n\nRNAsik\n fastq grouping works in two modes:\n\n\n\n\nsmart guessing it is a little involved but essentially it uses regular expression to check if fastq files have common suffix and therefore belong to the same sample. It heavily relies on clear labeling of R1 and R2 reads for paired-end data. \n\n\na more straight forward mode is simply to use samples sheet file, which is any text file with two columns separated by a tab character, \nold_prefix\\tnew_prefix\n. Prefix in this case is your sample name, unique bit of the file. \n\n\n\n\nSamples sheet in a bit more details; If you have four samples, two wild-type and two controls, you should have four bam files after the analysis. However you number of fastq files is rather variable, depending on your sequencing. For paired-end sequencing you are going to end up with 2 fastq files per sample and 8 fastq files all up. If your sequencing was also split across lanes, say two lanes, then you are going to have 4 fastq file per each samples and 16 fastq files in total. \nRNAsik\n tries to simplify this for you.\n\n\nRNAsik output\n\n\nDirectories breakdown\n\n\n\n\nDirectories\nDescription\n\n\nrefFiles/\n Contains the reference files (FASTA and GTF) and indices (aligner index) used in the analysis run \n\n\nalignerFiles/\n Containts all other, additional files from alignment run, but the bam files. e.g aligner specific log files, splice junction files \n\n\nbamFiles/\n Contains pre-processed BAM files, i.e sorted and duplicates marked as well as indexed, all using samtools and picard tools. These BAMs can be used in [IGV](http://software.broadinstitute.org/software/igv/) to view read alignments \n\n\ncountFiles/\n Contains read count files, \"raw\" - from `featureCounts`, degust ready counts and filtered for protein_coding features only\n\n\ncoverageFiles/\n Contains bigWig files for every bam (sample) file - from `bedtools genomecov` and `bedGrapToBigWig` USCS binary. Can load those into IGV\n\n\nfastqReport/\n Contains FastQC HTML reports for individual FASTQ files\n\n\nqualiMapResults/\n Contains int(ra|er)genic rates from each BAM file. Each BAM has its own directory with metric files. These results generated using `QualiMap rnaseq` command\n\n\nfastqDir/\n If you are going to pull your FASTQ file over http in tarball, then tarball will be unarchived here\n\n\nmultiqc_data/\nDirectory created by MultiQC holding a parsed text file, it doesn't serve any purpose for html file\n\n\nlogs/\nDirectory that holds subdirectories, self explanatory, with logs files\n\n\n\n\n\nFiles breakdown\n\n\n\n\nFiles\nDescription\n\n\ngeneIds.txt\n Hold four additional columns that get added into read counts file, that has postfix \"-withNames. Gene.id, Chrom, Gene.Name, Biotype.\n\n\nstrandInfo.txt\n Contains guesses, based on `featureCounts` `.summary` files, strand informataion\n\n\nmultiqc_report.html\nThis is the report file produced by MultiQC tool. A stand alone html file and can be viewed in any browser\n\n\n\n\n\nRNAsik output explained\n\n\n\n\nI hope that the directories and files naming is some what self explanatory, but here is a bit more detailed explanation of those.\n\n\n\n\nBam files\n\n\nFirst thing you most certainly going to get out of the pipeline is your bam files, those will be placed into \nbamFiles/\n directory. I don't really understand why, but \nfeatureCounts\n works best (fastest) with name sorted BAM files a.k.a unsorted. There is really two types of sorting, sorted by coordinates, often preferred as you can index those bam files and then have quick access to random parts of the file, second type is sorted by name, which insures that in paired-end experiment R1 and R2 pairs are interleaved, one after another, but you can't index those). \nSTAR\n aligner can output either of those files. I'm however outputting \"unsorted\" bam file and then in the second step sorting it with \npicard SamSort\n tool. There are a couple of reasons for that:\n\n\n\n\nother aligners don't sort e.g bwa and therefore assuming sorted bam file won't work well\n\n\neven though \nSTAR\n is pretty amazing (honestly), but I still rather prefer one tool for one job,\nhence why I also don't count reads with \nSTAR\n\n\n\n\nThe bam files from \nbamFiles/\n are only used with \nfeatureCounts\n and then \npicard\n suite converts them into sorted and marked duplicate bam files, which are now placed into \nmdupsFiles/\n directory. The rest of the analysis based on these bam files. I'm still deciding what to do with \"raw\" bam files in \nbamFiles/\n directory. They should be removed after run have finished, but if you have to re-run the pipeline to get additional things (which you can, it will resolve all dependencies and only run new tasks) those bams are now gone and will get regenerated, which will trigger the rest of pipeline to re-run, which is unwanted result. This is why I'm not auto-removing those bam files, rather doing manually after I'm sure.  \n\n\nCount files\n\n\nProbably the second most important thing in the pipeline is getting read counts. That is given some genome annotation count how many of mapped reads actually ended up mapping into know annotation. For classical differential expression analysis we are interested in protein coding genes only, which pipeline attempts to filter for, but there are other biotypes that we can differentially compare.\n\n\nThe pipeline attempts to guess the strand (directionality) of your library. In theory sequencing provider that had made your libraries should be able to tell you that, but sometimes they get it wrong or simply that information never reaches us (bioinformaticians) hence the guessing.\n\n\nPipelines runs \nfeatureCounts\n three times forcing reads to forward strand only, forcing to reverse strand only and allowing counting on both strand (non stranded library). \nfeatureCounts\n is very nice and it provides summary table that has number of assigned to feature reads. One can simply compare forward and reverse stranded counts and deduce the strand of the library. In essence this formula is used \nforward-reverse/forward+reverse\n to obtain the ration, if ration is about 0.9 then library is stranded and sign indicates the strand type, if however ration is about 0.1 then library is non stranded, anything else will indication undetermined and \nstrandInfo.txt\n file with default to \nNonStranded,1\n note the number one after the comment indicating status code, meaning exit with error. If you see that in your \nstrandInfo.txt\n file you'll need to manually inspect your \n*.summary\n files from \nfeatureCounts\n and make decision about which library type to go with. Actual implementation of strand guessing can be found in this script \nscripts/strand_guessing.py\n.\n\n\nfeatureCounts\n by default for any given run outputs two files, counts (e.g \nNonStrandedCounts.txt\n) and summary (e.g \nNonStrandedCounts.txt.summary\n). \nRNAsik\n attempts to \"clean up\" counts file, which includes removing and addition of certain columns to make counts files more informative. The columns that are added can be found in \ngeneIds.txt\n. If for what ever reason your \ngeneIds.txt\n is empty then all the other files with postfix \n-withNames\n going to be empty too. You could try to regenerate \ngeneIds.txt\n file using \nscripts/get_gene_ids.py\n script and then \nscripts/mk_counts_file.py\n to obtain \"clean\" table of counts. Doing so isn't strictly needed however additional information such as human understandable gene name and biotypy often come very handy in understanding differential expression. Having a biotype in counts file also allows you to filter for specific biotype e.g \nprotein_coding\n or \nsnRNA\n provided your annotation file has that information.\n\n\nCommand line options\n\n\nRead alignment\n\n\n\n\nOptions\nUsage\n\n\n-align\nspecify your aligner of choice [star|starWithAnn|hisat|bwa]\n\n\n-fqDir\nspecify path to your raw data directory. `RNAsik` will search that path recursively, so don't worry about nested directores\n\n\n-fastaRef\nspecify path to your reference FASTA file, i.e file that holds your refrence genome\n\n\n-paired\nspecify if data is paired end (RNASik looks for R1 and R2 in the FASTQ filename representing Read 1 and Read 2 \n\n\n\n\n\nRead counting\n\n\n\n\nOptions\n Usage \n\n\n-counts\n \n flag if you'd like to get read counts\n\n\n-gtfFile\n \n specify path to your reference annotation file [GTF|GFF|SAF]\n\n\n\n\n\nReads metrics\n\n\n\n\nOptions\n Usage \n\n\n-all\n \n This is an aggregate flag that is a short hand to get everything pipeline has to offer\n\n\n-qc\n \nFlag to collect several different QC metrics on your BAM and counts data\n\n\n-exonicRate\n \n flag if you'd like to get Int(ra|er)genic rates for your reads, using QualiMap tool\n\n\n-multiqc\n \n flag if you'd like to get general report that summarises different log files including `STAR`, `featureCounts`, `FastQC` and `QualiMap`\n\n\n\n\n\nExtra options\n\n\n\n\nOptions\n Usage \n\n\n-refFiles\n \n path to refFiles/ directory previously generated by RNAsik run\n\n\n-mdups\n \n flag to mark duplicates\n\n\n-trim\n \n perform FASTQ adapter and quality trimming\n\n\n-cov\n \n get coverage plots, bigWig \n\n\n-umi\n \n flag to deduplicate using UMI information\n\n\n-samplesSheet\n \n specify name of a tab separated text file, two columns,the first with old prefixes to be removed by new prefixes in the second column\n\n\n-genomeIdx\n \n specify path to pre-existing alignment index \n\n\n-outDir\ngive a name to your analysis output directory [sikRun] \n\n\n-extn\n \n provide your fastq files extntion. [\".fastq.gz\"]  \n\n\n-pairIds\n \n provide type identification, default is [`_R1,_R2`]\n\n\n-extraOpts\n \n provide key=value pairs, one per line, with key being tool name and value is a string of options e.g `star=\"--outWigType bedGraph\"` \n\n\n-configFile\nspecify your own config file with key=value pairs, one per line, for all tools\n\n\n\n\n\nUnusual user case\n\n\n\n\n-bamsDir\n \n specify path to BAMs directory. Use if bams were generated outside of the pipeline \n\n\n\n\n\nTweet to @kizza_a\n \n\n\n\n\n\n\nShare\n\n\n\n\n\nPython scripts\n\n\nThere a few python scripts that are used in the pipeline to do various things, mostly to do with read counts post processing.\nHopefully script name are self explanatory or have a good hint to what they do. All script have help menu, simply run the scripts\nwith \n-h|--help\n to get more information. All script also output to stdout (print to the screen) in order to save the output you'll\nneed to redirect to a file with \n symbol.\nAll script set to be executable so you should be able just run them. If for some reason you cannot run them just use \npython\n prefix\nin front of the script name. Here are some examples on how to run python scripts\n\n\n./get_geneids.py\n\n\n\n\nOR\n\n\n/full_path/to/get_geneids.py\n\n\n\n\nOR\n\n\npython get_geneids.py\n\n\n\n\n\n\nget_geneids.py\n\n\n\n\nConverts GTF|GFF files into four columns, tab delimited file \nGene.Id\\tChrom\\tGene.Name\\tBiotype\n\nNote that \nGene.Name\n and \nBiotype\n are subject to availability, i.e if that information isn't present\nin your annotation file then script (obviously) can't parse it out.\nWe need this information to augment our counts file. This isn't essential for DGE but makes\nour exploratory analysis easier.\n\n\n\n\nmk_cnts_file.py\n\n\n\n\nOnce we have our counts files from \nfeatureCounts\n and we've created \ngeneIds.txt\n file with previous command\nWe can add that extra meta information to our counts file.\nNote that this script filter on biotype, default is [protein_coding]. Also note that if biotype information is\nabsent then your \n-withNames-proteinCoding.txt\n file will be empty since no such biotype was found.\nYou can use \n--biotype all\n to get unfiltered - all genes. This is usually located in \n-withNames.txt\n file\n\n\n\n\nstrand_guessing.py\n\n\n\n\nThis script takes three different counts files i.e Forward, Reverse and Non stranded counts and attempts to\nguess which strand the data is. We can get this information from the sequencing facility and most Illumina\nlibrary preparation kits are reverse stranded these days, but sometimes things happened and this approach\ngives us correct answers from the data itself.\nThe output of this script is three values, comma separated; StrandType,StrandValue,ExitCode. e.g \nReverseStandedCounts,-0.924,0\n\nThe first value is obvious, tells you the type of strand it guessed. The second value is calculated like this\n\n\nstrnd_val = float(forward - reverse) / float(forward + reverse)\n\n\n\n\nwhich in essence allows us to estimate strand type. It is useful to look at that value as well.\nMagnitude is the confidence and the sign is directrion, negative is reverse, positive is forward stranded\nThe third value what's called an exit code, zero == all good. It means the script is pretty confident that it guessed it right. The other\npossible value could be 1, 1 == not good. It means the script couldn't guess what the strand was and simply\ndefaulted to \nNonStrandedCounts\n. You should never see exit code 1 with anything else by non stranded counts\ni.e \nNonStrandedCounts,1\n. If you do, please report this as a bug.\n\n\n\n\nmk_igv_links.py\n\n\n\n\nThis script isn't used in the pipeline actually, but it comes rather handy if you want to visualise your\ncoverage plots, bigWig (\n.bw\n) files or bam (\n.bam\n) files\n\n\nCase study\n\n\nOnce your \nRNAsik\n run had finished, have a look at \nstrandInfo.txt\n file\n\n\ncat sikRun/countFiles/strandInfo.txt\n\n\n\n\nIf you are seeing \nNonStrandedCounts,1\n you will need to manually check and figure out what strand your\ndata actually is. You will need to have a look at the \n.summary\n files for each of the three counts files.\nThis is standard, summary, output file from \nfeatureCounts\n. For simplicity sake just look at the first sample's\nraw counts, second column and we are just going to look at \"Assigned\" row, which is second row. So we are focusing\non second column, second row cell. We want to compare the number in that cell across three different strands counts.\n\n\nIf the data is stranded then that strand going to get dominant number of reads. If reverse stranded counts getting\nvastly more reads then forward stranded counts then the data is reverse stranded. It is possible that non stranded\ncounts in this case get essentially the same number of reads as reverse stranded. This is okay, but the data still is\nreverse stranded. Even if your non stranded counts have slightly more counts then your most dominant strand, (in this case)\nreverse stranded, this is still a reverse stranded data.\n\n\nOn the other hand if you are seeing roughly 50/50 split between forward and reverse reads, then this is non stranded\ndata.\n\n\nNow that you know what you are looking for, lets say, your data actually appears forward stranded and you would like to get\n\n-withNames.txt\n and \nwithNames-proteinCoding.txt\n files. You will need to use \nmk_cnts_file.py\n script\n\n\n./mk_cnt_file.py -h\n\nusage: mk_cnts_file.py --counts_dir \npath/to/coutns_dir\n --gene_ids \npath/to/geneIds.txt\n\nThis script summarises log files information into html table\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --counts_file COUNTS_FILE\n                        path to directory with featureCounts files\n  --gene_ids GENE_IDS   path to geneIds.txt file, format EnsmblId Chrm\n                        GeneName Biotype\n  --samples_sheet SAMPLES_SHEET\n                        path to samplesSheet.txt file, format old_prefix\n                        new_prefix\n  --biotype BIOTYPE     specify biotype of interest [protein_coding], 'all' is\n                        special to include everything\n  --counts_col COUNTS_COL\n                        Indicate which column begins read counts. default [7],\n                        which is featureCounts default, all columns before 7th\n                        are metadata cols\n\n\n\n\nTo get all feature types\n\n\n./mk_cnt_file.py --counts_file sikRun/countFiles/ForwardStrandedCounts.txt \\\n                 --gene_ids sikRun/countFiles/geneIds.txt \\\n                 --samples_sheet sikRun/samplesSheet.txt \\\n                 --biotype all \n ForwardStrandedCounts-withNames.txt\n\n\n\n\nTo get just protein_coding features\n\n\n./mk_cnt_file.py --counts_file sikRun/countFiles/ForwardStrandedCounts.txt \\\n                 --gene_ids sikRun/countFiles/geneIds.txt \\\n                 --samples_sheet sikRun/samplesSheet.txt \\\n                 --biotype protein_coding \n ForwardStrandedCounts-withNames-proteinCoding.txt\n\n\n\n\nMetric files explained\n\n\n\n\nA lot of the tools output some sort of metrics files, to stdout/stderr or a file.\nThose metrics are important for analysis and QC checks going forward with the data.\nAll of those metrics files are summarised into \nmultiqc\n report.\nThis section will attempt to explain in details each one of those metrics\n\n\n\n\nSTAR\n\n\n\n\nLog.final.out\n\n\n\n\nThis is straight out of the STAR docs\n\n\nLog.final.out: summary mapping statistics after mapping job is complete, very useful for\nquality control. The statistics are calculated for each read (single- or paired-end) and then\nsummed or averaged over all reads. Note that STAR counts a paired-end read as one read,\n(unlike the samtools flagstat/idxstats, which count each mate separately). Most of the informa-\ntion is collected about the UNIQUE mappers (unlike samtools flagstat/idxstats which does not\nseparate unique or multi-mappers). Each splicing is counted in the numbers of splices, which\nwould correspond to summing the counts in SJ.out.tab. The mismatch/indel error rates are\ncalculated on a per base basis, i.e. as total number of mismatches/indels in all unique mappers\ndivided by the total number of mapped bases.\n\n\nPicard\n\n\n\n\n\n\nCollectAlignmentSummaryMetrics\n\n\n\n\n\n\nCollectGcBiasMetrics\n\n\n\n\nMarkDuplicates\n\n\n\n\nThese are all picard tools that are used in the pipeline\n\n\n\n\nCreateSequenceDictionary\n\n\nSortSam\n\n\nMarkDuplicates\n\n\nBuildBamIndex\n\n\nCollectAlignmentSummaryMetrics\n\n\nCollectInsertSizeMetrics\n\n\nCollectGcBiasMetrics\n\n\nEstimateLibraryComplexity", 
            "title": "Docs"
        }, 
        {
            "location": "/docs/#documentation", 
            "text": "", 
            "title": "Documentation"
        }, 
        {
            "location": "/docs/#quick-start", 
            "text": "", 
            "title": "Quick start"
        }, 
        {
            "location": "/docs/#install", 
            "text": "conda config --add channels defaults\nconda config --add channels conda-forge\nconda config --add channels bioconda\nconda install -c serine rnasik \nconda install -c bioconda qualimap", 
            "title": "Install"
        }, 
        {
            "location": "/docs/#align-raw-reads", 
            "text": "RNAsik -align star \\\n       -fastaRef /path/to/reference.fasta \\\n       -fqDir /path/to/raw-data/directory", 
            "title": "Align raw reads"
        }, 
        {
            "location": "/docs/#count-gene-features", 
            "text": "RNAsik -counts \\\n       -gtfFile path/to/annotation.gtf", 
            "title": "Count gene features"
        }, 
        {
            "location": "/docs/#the-lot", 
            "text": "RNAsik -fqDir /path/to/raw-data/directory \\\n       -align star \\\n       -refFiles /path/to/refDir \\\n       -counts \\\n       -all", 
            "title": "The lot"
        }, 
        {
            "location": "/docs/#data-set-for-testing", 
            "text": "N.B  RNAsik  pipeline is some what resource hungry. This isn't  RNAsik  fault per say, because it \"simply\" wraps other tools. STAR aligner required fair amount of RAM and cpus. For a large genome like mouse it required around 30 Gb of RAM and the more cpus you have the quicker you'll map. I would advise not run the pipeline with less than 4 cores, which is default. This testing data set is of yeast and requires about 14 Gb of RAM. Also read  this comment , in summary you might have  RNAsik  failing on system that are under minimum system resources, this is work in progess and should be fixed in the future.   I figured that for testing you need smallish data set as well as species with a small genome, as indexing of genome takes a while for larger genome e.g mouse\nI found this study  GSE103004  which looks like an open access. If you follow  that link  you should hit front GEO page for that study. You can find your way to actual data (SRA files) files, but I always find it's a bit convoluted, so hit  here is a link to data files .   I've already prepared raw-data (fastq) files for you. I also reduced number of samples and sub-sampled reads to speed up your test run. Firstly though let me explain to you how to get full data set.    download sratoolkit  which is set of tools from  NCBI  that you'll need to download  sra files  and then extract/convert those to fastq files.    fastq-dump --gzip --split-files SRR3407195  this is a command that you'll want to run to get one particular sra file, not  --split-files  options, you need to use that if you data is paired end. If you don't use that flag, then you are going to end up with a single fastq file that has reads interleaved or truncated/merged in a funny way (had some issues like that in the past)    However you don't want run that command several times, so use a loop  while read s; do fastq-dump --gzip --split-files $s   $s.log 2 1  done   SRR_Acc_List.txt  You can download  SRR_Acc_List.txt file at this page  (mentioned that page before). That list has 9 sra files corresponding to 9 samples, where each samples was paired end and therefore total number of files is double - 18.   Also note that default marking when extracting from sra for R1 and R2 is  _1  and  _2  respectively and so if you are running  RNAsik  on that full data set you'll need to pass  -pairIds \"_1,_2\"  flag, default is  -pairIds \"_R1,_R2\"    If you want nicely labeled bam and then counts you can pass  -samplesSheet samplesSheet.txt . I haven't implemented url based samples sheets, so you'll need to download one before hand from  here . I'll include handling of url based samples sheets into roadmap, so watch that space !  If you ran  RNAsik  on a full data set and then used  Degust  for DGE analysis you should get  these results .  Note that report was generated using  pandoc  with custome template and  igv_links  were created using  mk_igv_links  script located in  scripts/  directory", 
            "title": "Data set for testing"
        }, 
        {
            "location": "/docs/#try-it-out", 
            "text": "RNAsik -align star \\\n       -fastaRef ftp://ftp.ensembl.org/pub/release-91/fasta/saccharomyces_cerevisiae/dna/Saccharomyces_cerevisiae.R64-1-1.dna_sm.toplevel.fa.gz \\\n       -fqDir http://bioinformatics.erc.monash.edu/home/kirill/sikTestData/rawData/IndustrialAntifoamAgentsYeastRNAseqData.tar \\\n       -counts \\\n       -gtfFile ftp://ftp.ensembl.org/pub/release-91/gtf/saccharomyces_cerevisiae/Saccharomyces_cerevisiae.R64-1-1.91.gtf.gz \\\n       -all \\\n       -paired", 
            "title": "Try it out"
        }, 
        {
            "location": "/docs/#introduction", 
            "text": "As mentioned previously in  about section  very first step in  RNA-seq analysis  is to map your raw reads ( FASTQ ) to the reference genome following by counting of reads that map onto a feature. But there is always more you could do with your data, in fact almost always only by doing more you can get deeper inside into your biological experiment and the system you are studying. And so  RNAsik  uses these tools to get as much out of your data as possible in an streamline run:   STAR aligner for mapping  featureCounts from subread package for read counting  samtools for coverage calculation and general bam files filtering  picard tools also for general bam fiels filtering  QualiMap for intragenic and interegenic rates  FastQC for QC metrics on yor fastq files  MultiQC for wraping everying into nice, single page report     As one can imagine every one of those tools has several number of options and by running  RNAsik-pipeline  you get predefined - subjective run. Obviously it all comes from years of experience and continues development and improvement. Use can always pass his/her own options through  -extraOptions  flag for more fine turning. \nAlternatively as, hinted above, user can leverage of  RNAsik  to run everything separately with fine control over the individual run.  RNAsik  produces  .html report  with all commands options specified.", 
            "title": "Introduction"
        }, 
        {
            "location": "/docs/#installation", 
            "text": "", 
            "title": "Installation"
        }, 
        {
            "location": "/docs/#using-conda", 
            "text": "download  miniconda   .sh  installer   run it and follow the prompts   bash Miniconda3-latest-Linux-x86_64.sh    add a few  conda  \"channels\", this is so  conda  knows where to get things from   conda config --add channels defaults\nconda config --add channels conda-forge\nconda config --add channels bioconda   install  RNAsik  pipeline   conda install -c serine rnasik    additionally install  qualimap  separately (it was a tricky to include qualimap into RNAsik because large number of dependencies that qualimap has)   conda install -c bioconda qualimap   Right now  RNAsik  hosted from my \"channel\" (conda terminology). There are plans to push it to official  bioconda channel", 
            "title": "Using conda"
        }, 
        {
            "location": "/docs/#conda-extras", 
            "text": "search \"main\" label (repository)   conda search -c serine rnasik   search \"dev\" label (repository)   conda search -c serine/label/dev rnasik   install specific version   conda install -c serine/label/dev  rnasik=1.5.1+3ecd215    install specific version and specific build number   conda install -c serine/label/dev  rnasik=1.5.1+3ecd215=4   simply install latest from that \"dev\" label (repository)   conda install -c serine/label/dev rnasik  NOTE:  that these are just some extras commands mainly for testing purposes not for production use !", 
            "title": "conda extras"
        }, 
        {
            "location": "/docs/#alternative-installation-methods", 
            "text": "HERE", 
            "title": "Alternative installation methods"
        }, 
        {
            "location": "/docs/#user-input", 
            "text": "", 
            "title": "User input"
        }, 
        {
            "location": "/docs/#reference-files", 
            "text": "Input File Description  FASTA file Most often this is your genomic reference sequence. It is a FASTA file holding raw DNA sequences where different features e.g chromosomes are labeled uniquely with a header line starting with '>'. [FASTA Format Description](https://en.wikipedia.org/wiki/FASTA_format)   GTF/GFF/SAF file  This is your gene annotation file (i.e coordinates of your genes, exons and other genomic features). This should be linked and associated with your genomic reference file. SAF (simple annotation format) is something that featureCounts use and it supported by the pipeline", 
            "title": "Reference files"
        }, 
        {
            "location": "/docs/#raw-data", 
            "text": "Input File Description  FASTQ file These are your raw files that are provided by the sequencing facility to you, they can be gzipped (.fq, .fastq, .fq.gz, .fastq.gz)", 
            "title": "Raw data"
        }, 
        {
            "location": "/docs/#user-input-explained", 
            "text": "", 
            "title": "User input explained"
        }, 
        {
            "location": "/docs/#annotation-files", 
            "text": "Annotation file would central for differential expression (DE) analysis without one you won't be able to do one. You could have very well assembled genome with very good mapping rate, but unless you know where your genes are on that genome i.e start and end coordinates for your features e.g genes you won't be able to deduce any information about those features and therefore compare between conditions. Below is an example of bear minimum information you need for feature counting.   GeneID  Chr Start   End Strand\n497097  chr1    3204563 3207049 -\n497097  chr1    3411783 3411982 -\n497097  chr1    3660633 3661579 -  There few entities that provide genome annotation, some cover more species than other. There will be of course individuals that simply provide annotation for one particular species, perhaps for more rare model organisms.  There are also different annotation file formats out there, which makes a little hard to provide  RNAsik  support for all of them. Currently  RNAsik  can only work with  GFF ,  GTF  or  SAF  file formats. There are many compatibilities issues between formats, but more importantly certain bits of information are only found in some of the files. The example above show  SAF  file format and as you can see that includes not human redable gene names nor biotype.  GFF  also often doesn't have biotype information, but on the other hand has product tag, which has short description, for protein coding at least, of resulting protein product,  GTF  lacks that information. Because of all these little nuances it can be hard to capture all of the desirable information.  Most tools in the pipeline prefer  GTF , some can only work with  GTF . I guess main reason for this is that every line is self contained and the format has been fairly predictable/stable.  If for whatever reason you can't get hold of  GFF/GTF  files and your annotation comes in  GenBank  (very common for bacterial genomes) or  Bed  files, don't panic and try to parse those files into  SAF  format. There are plans to include  gb_parse.py  script that should help most people with  GenBank  files.  Irrespective of which reference file distributor and which annotation file you are going to use, it is highly recommended that both of those files come from the same distributor. Most common distributors are  Ensembl ,  UCSC  and  NCBI .", 
            "title": "Annotation files"
        }, 
        {
            "location": "/docs/#raw-data-files", 
            "text": "Raw data is something that you should take good care of. You can regenerate all other data files, but you can't really regenerate you raw data, not unless you have lots of money and time. So be sure to back your  fastq  files up and never mess/do (i.e modify) your original fastq files. If you want to try something out, make a copy and do whatever you are doing on a copy. Also there will never be a need to unzip your fastq file. All of you fastq file should be gziped and have file extension  .fastq.gz  or  fq.gz  or something similar.   RNAsik  will search recursively your  -fqDir  and find all fastq files.  RNAsik  can handle nested directories as long as your data is homogeneous i.e all data belongs to the same library type single-end or paired-end. If data is paired end,  RNAsik  uses  -pairIds  value to figure out read pairs. You can check that all of your fastq files had been found by looking into  sikRun/logs/samples/fqFiles .   After obtaining a list of all fastq files  RNAsik  tries to be smart and attempts to group fastq files into samples, that is R1 and R2 reads are grouped, but also any fastq files that had been split across lanes should also be grouped. You should end up, after the run, with the same number of bam files as you have samples. Again you can check grouping in  sikRun/logs/samples/fqMap  RNAsik  fastq grouping works in two modes:   smart guessing it is a little involved but essentially it uses regular expression to check if fastq files have common suffix and therefore belong to the same sample. It heavily relies on clear labeling of R1 and R2 reads for paired-end data.   a more straight forward mode is simply to use samples sheet file, which is any text file with two columns separated by a tab character,  old_prefix\\tnew_prefix . Prefix in this case is your sample name, unique bit of the file.    Samples sheet in a bit more details; If you have four samples, two wild-type and two controls, you should have four bam files after the analysis. However you number of fastq files is rather variable, depending on your sequencing. For paired-end sequencing you are going to end up with 2 fastq files per sample and 8 fastq files all up. If your sequencing was also split across lanes, say two lanes, then you are going to have 4 fastq file per each samples and 16 fastq files in total.  RNAsik  tries to simplify this for you.", 
            "title": "Raw data files"
        }, 
        {
            "location": "/docs/#rnasik-output", 
            "text": "", 
            "title": "RNAsik output"
        }, 
        {
            "location": "/docs/#directories-breakdown", 
            "text": "Directories Description  refFiles/  Contains the reference files (FASTA and GTF) and indices (aligner index) used in the analysis run   alignerFiles/  Containts all other, additional files from alignment run, but the bam files. e.g aligner specific log files, splice junction files   bamFiles/  Contains pre-processed BAM files, i.e sorted and duplicates marked as well as indexed, all using samtools and picard tools. These BAMs can be used in [IGV](http://software.broadinstitute.org/software/igv/) to view read alignments   countFiles/  Contains read count files, \"raw\" - from `featureCounts`, degust ready counts and filtered for protein_coding features only  coverageFiles/  Contains bigWig files for every bam (sample) file - from `bedtools genomecov` and `bedGrapToBigWig` USCS binary. Can load those into IGV  fastqReport/  Contains FastQC HTML reports for individual FASTQ files  qualiMapResults/  Contains int(ra|er)genic rates from each BAM file. Each BAM has its own directory with metric files. These results generated using `QualiMap rnaseq` command  fastqDir/  If you are going to pull your FASTQ file over http in tarball, then tarball will be unarchived here  multiqc_data/ Directory created by MultiQC holding a parsed text file, it doesn't serve any purpose for html file  logs/ Directory that holds subdirectories, self explanatory, with logs files", 
            "title": "Directories breakdown"
        }, 
        {
            "location": "/docs/#files-breakdown", 
            "text": "Files Description  geneIds.txt  Hold four additional columns that get added into read counts file, that has postfix \"-withNames. Gene.id, Chrom, Gene.Name, Biotype.  strandInfo.txt  Contains guesses, based on `featureCounts` `.summary` files, strand informataion  multiqc_report.html This is the report file produced by MultiQC tool. A stand alone html file and can be viewed in any browser", 
            "title": "Files breakdown"
        }, 
        {
            "location": "/docs/#rnasik-output-explained", 
            "text": "I hope that the directories and files naming is some what self explanatory, but here is a bit more detailed explanation of those.", 
            "title": "RNAsik output explained"
        }, 
        {
            "location": "/docs/#bam-files", 
            "text": "First thing you most certainly going to get out of the pipeline is your bam files, those will be placed into  bamFiles/  directory. I don't really understand why, but  featureCounts  works best (fastest) with name sorted BAM files a.k.a unsorted. There is really two types of sorting, sorted by coordinates, often preferred as you can index those bam files and then have quick access to random parts of the file, second type is sorted by name, which insures that in paired-end experiment R1 and R2 pairs are interleaved, one after another, but you can't index those).  STAR  aligner can output either of those files. I'm however outputting \"unsorted\" bam file and then in the second step sorting it with  picard SamSort  tool. There are a couple of reasons for that:   other aligners don't sort e.g bwa and therefore assuming sorted bam file won't work well  even though  STAR  is pretty amazing (honestly), but I still rather prefer one tool for one job,\nhence why I also don't count reads with  STAR   The bam files from  bamFiles/  are only used with  featureCounts  and then  picard  suite converts them into sorted and marked duplicate bam files, which are now placed into  mdupsFiles/  directory. The rest of the analysis based on these bam files. I'm still deciding what to do with \"raw\" bam files in  bamFiles/  directory. They should be removed after run have finished, but if you have to re-run the pipeline to get additional things (which you can, it will resolve all dependencies and only run new tasks) those bams are now gone and will get regenerated, which will trigger the rest of pipeline to re-run, which is unwanted result. This is why I'm not auto-removing those bam files, rather doing manually after I'm sure.", 
            "title": "Bam files"
        }, 
        {
            "location": "/docs/#count-files", 
            "text": "Probably the second most important thing in the pipeline is getting read counts. That is given some genome annotation count how many of mapped reads actually ended up mapping into know annotation. For classical differential expression analysis we are interested in protein coding genes only, which pipeline attempts to filter for, but there are other biotypes that we can differentially compare.  The pipeline attempts to guess the strand (directionality) of your library. In theory sequencing provider that had made your libraries should be able to tell you that, but sometimes they get it wrong or simply that information never reaches us (bioinformaticians) hence the guessing.  Pipelines runs  featureCounts  three times forcing reads to forward strand only, forcing to reverse strand only and allowing counting on both strand (non stranded library).  featureCounts  is very nice and it provides summary table that has number of assigned to feature reads. One can simply compare forward and reverse stranded counts and deduce the strand of the library. In essence this formula is used  forward-reverse/forward+reverse  to obtain the ration, if ration is about 0.9 then library is stranded and sign indicates the strand type, if however ration is about 0.1 then library is non stranded, anything else will indication undetermined and  strandInfo.txt  file with default to  NonStranded,1  note the number one after the comment indicating status code, meaning exit with error. If you see that in your  strandInfo.txt  file you'll need to manually inspect your  *.summary  files from  featureCounts  and make decision about which library type to go with. Actual implementation of strand guessing can be found in this script  scripts/strand_guessing.py .  featureCounts  by default for any given run outputs two files, counts (e.g  NonStrandedCounts.txt ) and summary (e.g  NonStrandedCounts.txt.summary ).  RNAsik  attempts to \"clean up\" counts file, which includes removing and addition of certain columns to make counts files more informative. The columns that are added can be found in  geneIds.txt . If for what ever reason your  geneIds.txt  is empty then all the other files with postfix  -withNames  going to be empty too. You could try to regenerate  geneIds.txt  file using  scripts/get_gene_ids.py  script and then  scripts/mk_counts_file.py  to obtain \"clean\" table of counts. Doing so isn't strictly needed however additional information such as human understandable gene name and biotypy often come very handy in understanding differential expression. Having a biotype in counts file also allows you to filter for specific biotype e.g  protein_coding  or  snRNA  provided your annotation file has that information.", 
            "title": "Count files"
        }, 
        {
            "location": "/docs/#command-line-options", 
            "text": "", 
            "title": "Command line options"
        }, 
        {
            "location": "/docs/#read-alignment", 
            "text": "Options Usage  -align specify your aligner of choice [star|starWithAnn|hisat|bwa]  -fqDir specify path to your raw data directory. `RNAsik` will search that path recursively, so don't worry about nested directores  -fastaRef specify path to your reference FASTA file, i.e file that holds your refrence genome  -paired specify if data is paired end (RNASik looks for R1 and R2 in the FASTQ filename representing Read 1 and Read 2", 
            "title": "Read alignment"
        }, 
        {
            "location": "/docs/#read-counting", 
            "text": "Options  Usage   -counts    flag if you'd like to get read counts  -gtfFile    specify path to your reference annotation file [GTF|GFF|SAF]", 
            "title": "Read counting"
        }, 
        {
            "location": "/docs/#reads-metrics", 
            "text": "Options  Usage   -all    This is an aggregate flag that is a short hand to get everything pipeline has to offer  -qc   Flag to collect several different QC metrics on your BAM and counts data  -exonicRate    flag if you'd like to get Int(ra|er)genic rates for your reads, using QualiMap tool  -multiqc    flag if you'd like to get general report that summarises different log files including `STAR`, `featureCounts`, `FastQC` and `QualiMap`", 
            "title": "Reads metrics"
        }, 
        {
            "location": "/docs/#extra-options", 
            "text": "Options  Usage   -refFiles    path to refFiles/ directory previously generated by RNAsik run  -mdups    flag to mark duplicates  -trim    perform FASTQ adapter and quality trimming  -cov    get coverage plots, bigWig   -umi    flag to deduplicate using UMI information  -samplesSheet    specify name of a tab separated text file, two columns,the first with old prefixes to be removed by new prefixes in the second column  -genomeIdx    specify path to pre-existing alignment index   -outDir give a name to your analysis output directory [sikRun]   -extn    provide your fastq files extntion. [\".fastq.gz\"]    -pairIds    provide type identification, default is [`_R1,_R2`]  -extraOpts    provide key=value pairs, one per line, with key being tool name and value is a string of options e.g `star=\"--outWigType bedGraph\"`   -configFile specify your own config file with key=value pairs, one per line, for all tools", 
            "title": "Extra options"
        }, 
        {
            "location": "/docs/#unusual-user-case", 
            "text": "-bamsDir    specify path to BAMs directory. Use if bams were generated outside of the pipeline    Tweet to @kizza_a     \nShare", 
            "title": "Unusual user case"
        }, 
        {
            "location": "/docs/#python-scripts", 
            "text": "There a few python scripts that are used in the pipeline to do various things, mostly to do with read counts post processing.\nHopefully script name are self explanatory or have a good hint to what they do. All script have help menu, simply run the scripts\nwith  -h|--help  to get more information. All script also output to stdout (print to the screen) in order to save the output you'll\nneed to redirect to a file with   symbol.\nAll script set to be executable so you should be able just run them. If for some reason you cannot run them just use  python  prefix\nin front of the script name. Here are some examples on how to run python scripts  ./get_geneids.py  OR  /full_path/to/get_geneids.py  OR  python get_geneids.py   get_geneids.py   Converts GTF|GFF files into four columns, tab delimited file  Gene.Id\\tChrom\\tGene.Name\\tBiotype \nNote that  Gene.Name  and  Biotype  are subject to availability, i.e if that information isn't present\nin your annotation file then script (obviously) can't parse it out.\nWe need this information to augment our counts file. This isn't essential for DGE but makes\nour exploratory analysis easier.   mk_cnts_file.py   Once we have our counts files from  featureCounts  and we've created  geneIds.txt  file with previous command\nWe can add that extra meta information to our counts file.\nNote that this script filter on biotype, default is [protein_coding]. Also note that if biotype information is\nabsent then your  -withNames-proteinCoding.txt  file will be empty since no such biotype was found.\nYou can use  --biotype all  to get unfiltered - all genes. This is usually located in  -withNames.txt  file   strand_guessing.py   This script takes three different counts files i.e Forward, Reverse and Non stranded counts and attempts to\nguess which strand the data is. We can get this information from the sequencing facility and most Illumina\nlibrary preparation kits are reverse stranded these days, but sometimes things happened and this approach\ngives us correct answers from the data itself.\nThe output of this script is three values, comma separated; StrandType,StrandValue,ExitCode. e.g  ReverseStandedCounts,-0.924,0 \nThe first value is obvious, tells you the type of strand it guessed. The second value is calculated like this  strnd_val = float(forward - reverse) / float(forward + reverse)  which in essence allows us to estimate strand type. It is useful to look at that value as well.\nMagnitude is the confidence and the sign is directrion, negative is reverse, positive is forward stranded\nThe third value what's called an exit code, zero == all good. It means the script is pretty confident that it guessed it right. The other\npossible value could be 1, 1 == not good. It means the script couldn't guess what the strand was and simply\ndefaulted to  NonStrandedCounts . You should never see exit code 1 with anything else by non stranded counts\ni.e  NonStrandedCounts,1 . If you do, please report this as a bug.   mk_igv_links.py   This script isn't used in the pipeline actually, but it comes rather handy if you want to visualise your\ncoverage plots, bigWig ( .bw ) files or bam ( .bam ) files", 
            "title": "Python scripts"
        }, 
        {
            "location": "/docs/#case-study", 
            "text": "Once your  RNAsik  run had finished, have a look at  strandInfo.txt  file  cat sikRun/countFiles/strandInfo.txt  If you are seeing  NonStrandedCounts,1  you will need to manually check and figure out what strand your\ndata actually is. You will need to have a look at the  .summary  files for each of the three counts files.\nThis is standard, summary, output file from  featureCounts . For simplicity sake just look at the first sample's\nraw counts, second column and we are just going to look at \"Assigned\" row, which is second row. So we are focusing\non second column, second row cell. We want to compare the number in that cell across three different strands counts.  If the data is stranded then that strand going to get dominant number of reads. If reverse stranded counts getting\nvastly more reads then forward stranded counts then the data is reverse stranded. It is possible that non stranded\ncounts in this case get essentially the same number of reads as reverse stranded. This is okay, but the data still is\nreverse stranded. Even if your non stranded counts have slightly more counts then your most dominant strand, (in this case)\nreverse stranded, this is still a reverse stranded data.  On the other hand if you are seeing roughly 50/50 split between forward and reverse reads, then this is non stranded\ndata.  Now that you know what you are looking for, lets say, your data actually appears forward stranded and you would like to get -withNames.txt  and  withNames-proteinCoding.txt  files. You will need to use  mk_cnts_file.py  script  ./mk_cnt_file.py -h\n\nusage: mk_cnts_file.py --counts_dir  path/to/coutns_dir  --gene_ids  path/to/geneIds.txt\n\nThis script summarises log files information into html table\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --counts_file COUNTS_FILE\n                        path to directory with featureCounts files\n  --gene_ids GENE_IDS   path to geneIds.txt file, format EnsmblId Chrm\n                        GeneName Biotype\n  --samples_sheet SAMPLES_SHEET\n                        path to samplesSheet.txt file, format old_prefix\n                        new_prefix\n  --biotype BIOTYPE     specify biotype of interest [protein_coding], 'all' is\n                        special to include everything\n  --counts_col COUNTS_COL\n                        Indicate which column begins read counts. default [7],\n                        which is featureCounts default, all columns before 7th\n                        are metadata cols  To get all feature types  ./mk_cnt_file.py --counts_file sikRun/countFiles/ForwardStrandedCounts.txt \\\n                 --gene_ids sikRun/countFiles/geneIds.txt \\\n                 --samples_sheet sikRun/samplesSheet.txt \\\n                 --biotype all   ForwardStrandedCounts-withNames.txt  To get just protein_coding features  ./mk_cnt_file.py --counts_file sikRun/countFiles/ForwardStrandedCounts.txt \\\n                 --gene_ids sikRun/countFiles/geneIds.txt \\\n                 --samples_sheet sikRun/samplesSheet.txt \\\n                 --biotype protein_coding   ForwardStrandedCounts-withNames-proteinCoding.txt", 
            "title": "Case study"
        }, 
        {
            "location": "/docs/#metric-files-explained", 
            "text": "A lot of the tools output some sort of metrics files, to stdout/stderr or a file.\nThose metrics are important for analysis and QC checks going forward with the data.\nAll of those metrics files are summarised into  multiqc  report.\nThis section will attempt to explain in details each one of those metrics", 
            "title": "Metric files explained"
        }, 
        {
            "location": "/docs/#star", 
            "text": "Log.final.out   This is straight out of the STAR docs  Log.final.out: summary mapping statistics after mapping job is complete, very useful for\nquality control. The statistics are calculated for each read (single- or paired-end) and then\nsummed or averaged over all reads. Note that STAR counts a paired-end read as one read,\n(unlike the samtools flagstat/idxstats, which count each mate separately). Most of the informa-\ntion is collected about the UNIQUE mappers (unlike samtools flagstat/idxstats which does not\nseparate unique or multi-mappers). Each splicing is counted in the numbers of splices, which\nwould correspond to summing the counts in SJ.out.tab. The mismatch/indel error rates are\ncalculated on a per base basis, i.e. as total number of mismatches/indels in all unique mappers\ndivided by the total number of mapped bases.", 
            "title": "STAR"
        }, 
        {
            "location": "/docs/#picard", 
            "text": "CollectAlignmentSummaryMetrics    CollectGcBiasMetrics   MarkDuplicates   These are all picard tools that are used in the pipeline   CreateSequenceDictionary  SortSam  MarkDuplicates  BuildBamIndex  CollectAlignmentSummaryMetrics  CollectInsertSizeMetrics  CollectGcBiasMetrics  EstimateLibraryComplexity", 
            "title": "Picard"
        }, 
        {
            "location": "/help/", 
            "text": "RNAsik? yep!\n\n\nGetting help\n\n\nThere is an open \nRNAsik user's google group\n any can ask and answer\nquestions there. I will try my best to respond to any questions there, but bear in mind that I could get saturated with work\nand might not be able to respond straight away.\n\n\nAlso couple of house keeping things:\n\n\n\n\nbe polite (in general) and considerate of others.\n\n\nInclude as much information about your problem as you can.\nThe problem needs to be reproducible, otherwise I might not be able to help you.\n\n\n\n\np.s any feedback and suggestions are also welcomed there\n\n\nBug reports\n\n\nBest place to submit bugs is with \nGitHub issues\n\nTry to include as much information as you can and again problem needs to be reproducible. Sometimes problem could be \nBigDataScript specific so also try looking at \nBigDataScrip user's group", 
            "title": "Help"
        }, 
        {
            "location": "/help/#rnasik-yep", 
            "text": "", 
            "title": "RNAsik? yep!"
        }, 
        {
            "location": "/help/#getting-help", 
            "text": "There is an open  RNAsik user's google group  any can ask and answer\nquestions there. I will try my best to respond to any questions there, but bear in mind that I could get saturated with work\nand might not be able to respond straight away.  Also couple of house keeping things:   be polite (in general) and considerate of others.  Include as much information about your problem as you can.\nThe problem needs to be reproducible, otherwise I might not be able to help you.   p.s any feedback and suggestions are also welcomed there", 
            "title": "Getting help"
        }, 
        {
            "location": "/help/#bug-reports", 
            "text": "Best place to submit bugs is with  GitHub issues \nTry to include as much information as you can and again problem needs to be reproducible. Sometimes problem could be \nBigDataScript specific so also try looking at  BigDataScrip user's group", 
            "title": "Bug reports"
        }, 
        {
            "location": "/contrib/", 
            "text": "Contributing\n\n\nThere are many places for contribution the most obvious ones are help with documentations, help in the \nuser's group\n\nand of course with the source itself.\n\n\nDocumentations\n\n\nI'm using \nmkdocs\n to generate this site, which has been very easy to use.\nAll documentations are written in plain markdown and located in main repo \ndocs/\n directory\n. You can simply fork \nRNAsik\n repository, do changes to the docs and send me a pull request (PR). Any changes are super welcomed, even one letter spell correction (there'll be more than one), but all changes need to come through PR, which will not only acknowledge you as contributor, but also enable me to review changes quickly and incorporate them in (pull them in) easily.\n\n\nQuick notes on \nmkdocs\n, it is pretty easy to install with \npip\n in \nvirtualenv\n if you prefer (you should).\n\n\n\n\nto install mkdocs (don't have to use \nvirtualenv\n)\n\n\n\n\nvirtualenv mkdocs_env\nsource mkdocs_env/bin/activate\npip install mkdocs\n\n\n\n\n\n\nmkdocs in the nutshell\n\n\n\n\nmkdocs build\nmkdocs gh-deploy\n\n\n\n\nThis will deploy your copy of \nRNAsik\n docs to your \ngithub-pages (gh-pages)\n\nYou actually don't need to do that, you don't need deploy your own copy of the docs to your branch. Just use \nmkdocs\nserver\n (read below) to prerview changes and send them through to me.\n\n\ngit clone https://github.com/MonashBioinformaticsPlatform/RNAsik-pipe\ncd RNAsik-pipe\n# do docs changes\n\n\n\n\n\n\nget localhost server (to preview your changes)\n\n\n\n\nmkdocs server\n\n\n\n\nThis will give you live updates to you copy of the docs, default URL should be \nlocalhost:8000\n, but it will tell you that once you've started the server. Then simply use your favourite text editor to edit markdown documents. Commit your changes, don't be afraid to be verbose, say what you've added/changed/removed in your commit message. And send me PR.\n\n\nUser's group\n\n\nJust jump in and do it!\n\n\nDeveloping pipeline further\n\n\nI need to write a more comprehensive developer guide at sometime soon. Any contributions are again extremely welcomed and again as I've mentioned in the \ndocumentations\n section above, any contributions need to come through pull request (PR).\n\n\nTo summarise briefly layouts of the \nsrc/\n:\n\n\n\n\nRNAsik.bds\n is the main \"executable\" file that sources all required modules and runs the pipeline.\n\n\nsikHeader.bds\n defines help menu and all user inputs options. I do have a couple of command line\narguments hidden from main help menu, but if you take a pick at this file you'll see them all\n\n\nAll other \n*.bds\n files contain functions to specific tasks those functions get called in \nRNAsik.bds\n\n\n\n\nBuilding conda package\n\n\nFirst of all you need to set up your conda environment. If you don't have \nconda\n installed get it first.\n\n\n\n\ndownload \nminiconda\n \n.sh\n installer\n\n\nrun it and follow the prompts\n\n\n\n\nbash Miniconda3-latest-Linux-x86_64.sh\n\n\n\n\nThese are fairly routine steps, but if this is your first time you'll need to do them\n\n\n\n\nadd a few \nconda\n \"channels\", this is so \nconda\n knows where to get things from\n\n\n\n\nconda config --add channels defaults\nconda config --add channels conda-forge\nconda config --add channels bioconda\n\n\n\n\n\n\ninstall a couple of required \nconda\n packages\n\n\n\n\nconda install conda-build\nconda install anaconda-client\n\n\n\n\nNote that you can use \n-y\n flag to say assume \nyes\n instead of manually entering\nyes/no\n\n\n\n\nyou will need a copy of bioconda recipes. I haven't PR my fork to official bioconda channel\nso for now it is\n\n\n\n\ngit clone  https://github.com/serine/bioconda-recipes\ncd bioconda-recipes\nconda build recipes/rnasik\n\n\n\n\n\n\nTo install \nRNAsik\n locally from just build package. You need these two commands.\nFirst command simply list the location of where the \n.tar.bz2\n file is on the system.\nYou also need that location if you want to publish to anaconda repository.\nThe second command simply installs the package\n\n\n\n\nconda build recipes/rnasik --output\nconda install -y --use-local rnasik\n\n\n\n\n\n\nTo upload newly build package to anacoda repository\n\n\nset up an account at \nAnacoda\n\n\nanaconda login\n\n\nanaconda upload \npath_to_file.tar.bz2\n\n\nanaconda upload \npath_to_file.tar.bz2\n --label dev\n\n\n\n\n\n\n\n\nOnce you've logged in once, anaconda will store login token somewhere in your home directory\n\n\n\n\nhere ?\n\n\n\n\n~/.continuum/anaconda-client\n\n\n\n\nTravis CI and testing\n\n\nContinues integration is very useful to ensure your code is checked continiouslly. RNAsik code is checked (tested)\nwith every commit. However that testing only as good as I, or hopefully we, will make it. BigDataScript provides\nvery nice unit testing mechanism, the trick of course it gotta to be written. Currently only very small proportion\nof the code is actually covered by tests. A lot of work is needed in this space. Of course one might say that I should\nhave been writing tests as I was writing my code. Perhaps, but I'm new to this and better later then never!\n\n\nHave a look at bds docs\n on how to write tests.\n\n\nTweet to @kizza_a\n \n\n\n\n\n\n\nShare", 
            "title": "Contributing"
        }, 
        {
            "location": "/contrib/#contributing", 
            "text": "There are many places for contribution the most obvious ones are help with documentations, help in the  user's group \nand of course with the source itself.", 
            "title": "Contributing"
        }, 
        {
            "location": "/contrib/#documentations", 
            "text": "I'm using  mkdocs  to generate this site, which has been very easy to use.\nAll documentations are written in plain markdown and located in main repo  docs/  directory . You can simply fork  RNAsik  repository, do changes to the docs and send me a pull request (PR). Any changes are super welcomed, even one letter spell correction (there'll be more than one), but all changes need to come through PR, which will not only acknowledge you as contributor, but also enable me to review changes quickly and incorporate them in (pull them in) easily.  Quick notes on  mkdocs , it is pretty easy to install with  pip  in  virtualenv  if you prefer (you should).   to install mkdocs (don't have to use  virtualenv )   virtualenv mkdocs_env\nsource mkdocs_env/bin/activate\npip install mkdocs   mkdocs in the nutshell   mkdocs build\nmkdocs gh-deploy  This will deploy your copy of  RNAsik  docs to your  github-pages (gh-pages) \nYou actually don't need to do that, you don't need deploy your own copy of the docs to your branch. Just use  mkdocs\nserver  (read below) to prerview changes and send them through to me.  git clone https://github.com/MonashBioinformaticsPlatform/RNAsik-pipe\ncd RNAsik-pipe\n# do docs changes   get localhost server (to preview your changes)   mkdocs server  This will give you live updates to you copy of the docs, default URL should be  localhost:8000 , but it will tell you that once you've started the server. Then simply use your favourite text editor to edit markdown documents. Commit your changes, don't be afraid to be verbose, say what you've added/changed/removed in your commit message. And send me PR.", 
            "title": "Documentations"
        }, 
        {
            "location": "/contrib/#users-group", 
            "text": "Just jump in and do it!", 
            "title": "User's group"
        }, 
        {
            "location": "/contrib/#developing-pipeline-further", 
            "text": "I need to write a more comprehensive developer guide at sometime soon. Any contributions are again extremely welcomed and again as I've mentioned in the  documentations  section above, any contributions need to come through pull request (PR).  To summarise briefly layouts of the  src/ :   RNAsik.bds  is the main \"executable\" file that sources all required modules and runs the pipeline.  sikHeader.bds  defines help menu and all user inputs options. I do have a couple of command line\narguments hidden from main help menu, but if you take a pick at this file you'll see them all  All other  *.bds  files contain functions to specific tasks those functions get called in  RNAsik.bds", 
            "title": "Developing pipeline further"
        }, 
        {
            "location": "/contrib/#building-conda-package", 
            "text": "First of all you need to set up your conda environment. If you don't have  conda  installed get it first.   download  miniconda   .sh  installer  run it and follow the prompts   bash Miniconda3-latest-Linux-x86_64.sh  These are fairly routine steps, but if this is your first time you'll need to do them   add a few  conda  \"channels\", this is so  conda  knows where to get things from   conda config --add channels defaults\nconda config --add channels conda-forge\nconda config --add channels bioconda   install a couple of required  conda  packages   conda install conda-build\nconda install anaconda-client  Note that you can use  -y  flag to say assume  yes  instead of manually entering\nyes/no   you will need a copy of bioconda recipes. I haven't PR my fork to official bioconda channel\nso for now it is   git clone  https://github.com/serine/bioconda-recipes\ncd bioconda-recipes\nconda build recipes/rnasik   To install  RNAsik  locally from just build package. You need these two commands.\nFirst command simply list the location of where the  .tar.bz2  file is on the system.\nYou also need that location if you want to publish to anaconda repository.\nThe second command simply installs the package   conda build recipes/rnasik --output\nconda install -y --use-local rnasik   To upload newly build package to anacoda repository  set up an account at  Anacoda  anaconda login  anaconda upload  path_to_file.tar.bz2  anaconda upload  path_to_file.tar.bz2  --label dev     Once you've logged in once, anaconda will store login token somewhere in your home directory   here ?   ~/.continuum/anaconda-client", 
            "title": "Building conda package"
        }, 
        {
            "location": "/contrib/#travis-ci-and-testing", 
            "text": "Continues integration is very useful to ensure your code is checked continiouslly. RNAsik code is checked (tested)\nwith every commit. However that testing only as good as I, or hopefully we, will make it. BigDataScript provides\nvery nice unit testing mechanism, the trick of course it gotta to be written. Currently only very small proportion\nof the code is actually covered by tests. A lot of work is needed in this space. Of course one might say that I should\nhave been writing tests as I was writing my code. Perhaps, but I'm new to this and better later then never!  Have a look at bds docs  on how to write tests.  Tweet to @kizza_a     \nShare", 
            "title": "Travis CI and testing"
        }, 
        {
            "location": "/roadmap/", 
            "text": "Roadmap\n\n\nGoing forward\n\n\n1.x.x\n\n\n\n\nneed to have better way to check for index directory, want to know if starIdx includes or doesn't indexing with annotation. \n\n\nrecheck \nStuart's PR\n, polish off refFiles detection\n\n\nrecheck \nthis whole PR\n\n\nnoticed that qualimap could have high RAM consumption, need to fix cpu and mem parameters passing through sik.config file. Reckon to set mem at 4 or 6 Gb\n\n\n\n\n1.5.2 (June/July) 2018\n\n\n\n\nstart including unit testing in your master branch, once I'm happy with with \nunit_test\n branch\n\n\nmake coverage plots stranded. Use \n-strand\n flag for stranded data, but still output non-stranded coverage. So 3 files\n  all up\n\n\nadd support for another aligner - \nminimap2\n\n\n\n\n1.5.2 (July/August) 2018\n\n\n\n\nimprove \nRNAsik\n logging, particular want to make individual tools version logging independent of each other, so that if one wants to run\njust counts, \nRNAsik\n shouldn't complain about \nbwa\n not found in the PATH. Also double check the behaviour of the logger when pipelines re-runs.\nFrom memory it might not perform as it should. You really want a log of every event that had happened.\n\n\ninclude logging of split lanes and R1 and R2. Want to be able to see from the log whether two reads were classified as split lanes or paired end. This is\nto do with recent bug that got fixed in \nb924027\n \n\n\nhave a look at BDS \nlog()\n function as well, might come useful\n\n\n\n\n1.5.3 (October/November) 2018\n\n\nNew feature(s) described below:\n\n\n\n\nimplement new flag \n-fqFiles\n that can take either a file or a directory.\n\n\nif a directory is given, do what \n-fqDir\n does now and traverse down retuning list of fastq files. \n\n\nif a file is given, use those locations getting fastq files. Location can be local file path or URLs, assume one location per line\n\n\n\n\n\n\n\n\nWill keep \n-fqDir\n flag, as a backward compatibility with a warning that flag had been deprecated. Will also schedule to remove \n-fqDir\n completely in future releases\nBecause of changes in arg's options will do a minor version bump.\n\n\n\n\ninclude handling of url based samples sheets, i.e \n-samplesSheet\n flag should handle local based or remote files, just like \n-fastaRef\n option\n\n\n\n\n1.6.0 (October/November) 2018\n\n\n\n\n\n\nPlans to add variants calling to \nRNAsik\n. It'll be opt in flag,  \n-varsCall\n. Suggestions are welcomed about different name for a flag.\nI already have a prototype in bds, just need to plug it in.\n\n\n\n\nNot sure which caller to use \nGATK\n or \nfreebayes\n will need to do more reading on that. \n\n\nAlso need to document common pitfalls for using RNAseq for variant calling e.g can only can variances in coding regions that are expressed. \n\n\nNot a good idea to use pulled samples as you won't be able to get allele frequency\n\n\nwill also need to find out where to get known SNP's (snpDB? for known germ line mutations) and a list of blacklisted regions\n\n\n\n\n\n\n\n\nan example of someone else variant calling pipeline\n\n\n\n\n\n\nIdeas for future releases\n\n\n\n\ninclude IGVlink into RNAsik-pipe output, can only do that if data outputted into something that is hostable i.e object store?\n\n\nNeed better support for exonic/intronic rates estimation. Is \nread_distribution.py\n from RSeQC good idea? Right now qualiMap is ok flag to opt in.\n\n\nadd alignment free support for RNAseq analysis e.g salmon/kalisto\n\n\nis there need for circular RNA support?\n\n\n\n\nChangelog\n\n\n1.5.1\n\n\n\n\nseveral bug fixes including \n#18\n, hisat2 related bugs, and samtools sort memory bug\n\n\nin relation to bug \n#18\n added extra \nsanity check\n\n\ngeneral code improvement and maintenance\n\n\nupdated docs and included new installation method\n\n\nimproved python scripts, made them user executable and updated docs with how to use them\n\n\nupdated UCSC binaries due to libpng12 issue\n\n\n\n\n1.5.0\n\n\n\n\nfixed bugs and improved python scripts, also migrated them to python3\n\n\nfixed issues: #16, #15, #20, #21\n\n\nfixed bug in handling remote reference files\n\n\nadded \nmk_igv_links.py\n script as general utility script\n\n\nadded FASTQ trimmer - \nskewer\n\n\ninternal code improvements, made code more readable and clean\n\n\nRe-wrote bam files processing, i.e sorting and marking duplicates. Changed sorting from \npicard SortSam\n to \nsamtools sort\n. Kept \npicard MarkDuplicates\n for conventional marking duplicates, but add \nJe-suite\n for UMI based de-duplication.\n\n\nMade improvements in handling of featureCounts output for multiqc purposes.\n\n\ncompletely removed threads and memory parameters, now each task's gets it's own cpu and mem setting all through sik.config. This makes it more cluster friendly as well\n\n\n\n\nAdded more metrics gathering; samtools qc metrics:\n\n\n\n\nflagstat\n\n\nidxstats\n\n\nstats\n\n\n\n\n\n\n\n\nImproved many different task's dependencies flow.\n\n\n\n\n\n\nchanged few command lines options; new options:\n\n\n\n\n-all\n\n\n-counts\n\n\n-mdups\n\n\n-qc\n\n\n-umi\n\n\n-trim\n\n\n\n\n\n\n\n\nremoved command line options:\n\n\n\n\n-metrics\n\n\n-fastqc\n\n\n-prePro\n\n\n-threads\n\n\n-memory\n\n\n\n\n\n\n\n\nall flags changes have some backward compatibility, i.e will set the closet new option. \n\n\n\n\n\n\n1.4.9\n\n\n\n\nFixed BWA indexing problem. Problem around STAR and Hisat2 aligners pass index as directory whereas bwa as a file, had to fight to make all different options i.e \n-refFiles\n and \n-genomeIdx\n to work\n\n\nIncluded proper support for SAF file format handled through -gtfFile flag i.e user can pass GTF, GFF or SAF through that flag and will still get counts\n\n\nImporved handling of urls for refrence files \n-fastaRef\n, \n-gtfFile\n now works with all bds supported url types. Also fixed tarball url handling through \n-fqDir\n\n\nImproved code readability in several places and included \ncanFail\n flag for making degust reads counts files.\n\n\nFixed a bug in \nexonicRates\n function was passing \"wrong\" gtf file path\n\n\nUpdated docs, added more explanation on how RNAsik ticks. Included a roadmap to allow better time and features management\n\n\nfixed \n-pairIds\n bug, courtesy @stu2 (PR #6) and samples sheet file making\n\n\nImproved python script, including several small bug fixing\n\n\n\n\n1.4.8\n\n\n\n\nadded new feature: coverage plots generation\n\n\nadded new feature: ability to pass previously generated references directory, saves spaces and time\n\n\nfixed strand guessing script, should be better at guessing now\n\n\nmoved to \nmkdocs\n for documents compilation and deployment to gh_pages\n\n\nadded new python script to make degust file, this simplifies and strengthens the code\n\n\nimproved readability of the code\n\n\nimproved and fixed bugs in handling fastq files and assignment of fastq to sample names\n\n\n\n\n1.4.7\n\n\n\n\nfixed STAR memory allocation issue, now user can run with fewer cpus without a worry for STAR spawning multiple tasks causing out of memory issue.\n\n\nmade BASH wrapper (not ideal) for \nRNAsik\n this is capture \nbds\n logs including report.html which is rather valuable piece of information about the run\n\n\nintroduced another new aligner - \nbwa mem\n to do bacterial RNAseq.\n\n\nintroduced multiqc config file in \nconfigs\n directory now\n\n\ncompletely removed \n-fqRegex\n and added sanity check for paired end data. If R2 is found and -paired isn't set or vice verse then error message sent\n\n\nseveral general bug fixes\n\n\nsimplified help menu\n\n\n\n\n1.4.6\n\n\n\n\ngeneralised aligner's call, this will make it easier to add new aligners into RNAsik\n\n\nincluded support for \nhista2\naligner\n\n\nadded samplesSheep logging\n\n\nfixed few minor bugs and improved code quality\n\n\nadded \ncanFail\n option to several non crucial tasks, allowing pipeline to continue if some task failed of fastqc and qualimap to allow them to fail as those are non essential tasks.\n\n\n\n\nTweet to @kizza_a\n \n\n\n\n\n\n\nShare", 
            "title": "Roadmap"
        }, 
        {
            "location": "/roadmap/#roadmap", 
            "text": "", 
            "title": "Roadmap"
        }, 
        {
            "location": "/roadmap/#going-forward", 
            "text": "", 
            "title": "Going forward"
        }, 
        {
            "location": "/roadmap/#1xx", 
            "text": "need to have better way to check for index directory, want to know if starIdx includes or doesn't indexing with annotation.   recheck  Stuart's PR , polish off refFiles detection  recheck  this whole PR  noticed that qualimap could have high RAM consumption, need to fix cpu and mem parameters passing through sik.config file. Reckon to set mem at 4 or 6 Gb", 
            "title": "1.x.x"
        }, 
        {
            "location": "/roadmap/#152-junejuly-2018", 
            "text": "start including unit testing in your master branch, once I'm happy with with  unit_test  branch  make coverage plots stranded. Use  -strand  flag for stranded data, but still output non-stranded coverage. So 3 files\n  all up  add support for another aligner -  minimap2", 
            "title": "1.5.2 (June/July) 2018"
        }, 
        {
            "location": "/roadmap/#152-julyaugust-2018", 
            "text": "improve  RNAsik  logging, particular want to make individual tools version logging independent of each other, so that if one wants to run\njust counts,  RNAsik  shouldn't complain about  bwa  not found in the PATH. Also double check the behaviour of the logger when pipelines re-runs.\nFrom memory it might not perform as it should. You really want a log of every event that had happened.  include logging of split lanes and R1 and R2. Want to be able to see from the log whether two reads were classified as split lanes or paired end. This is\nto do with recent bug that got fixed in  b924027    have a look at BDS  log()  function as well, might come useful", 
            "title": "1.5.2 (July/August) 2018"
        }, 
        {
            "location": "/roadmap/#153-octobernovember-2018", 
            "text": "New feature(s) described below:   implement new flag  -fqFiles  that can take either a file or a directory.  if a directory is given, do what  -fqDir  does now and traverse down retuning list of fastq files.   if a file is given, use those locations getting fastq files. Location can be local file path or URLs, assume one location per line     Will keep  -fqDir  flag, as a backward compatibility with a warning that flag had been deprecated. Will also schedule to remove  -fqDir  completely in future releases\nBecause of changes in arg's options will do a minor version bump.   include handling of url based samples sheets, i.e  -samplesSheet  flag should handle local based or remote files, just like  -fastaRef  option", 
            "title": "1.5.3 (October/November) 2018"
        }, 
        {
            "location": "/roadmap/#160-octobernovember-2018", 
            "text": "Plans to add variants calling to  RNAsik . It'll be opt in flag,   -varsCall . Suggestions are welcomed about different name for a flag.\nI already have a prototype in bds, just need to plug it in.   Not sure which caller to use  GATK  or  freebayes  will need to do more reading on that.   Also need to document common pitfalls for using RNAseq for variant calling e.g can only can variances in coding regions that are expressed.   Not a good idea to use pulled samples as you won't be able to get allele frequency  will also need to find out where to get known SNP's (snpDB? for known germ line mutations) and a list of blacklisted regions     an example of someone else variant calling pipeline", 
            "title": "1.6.0 (October/November) 2018"
        }, 
        {
            "location": "/roadmap/#ideas-for-future-releases", 
            "text": "include IGVlink into RNAsik-pipe output, can only do that if data outputted into something that is hostable i.e object store?  Need better support for exonic/intronic rates estimation. Is  read_distribution.py  from RSeQC good idea? Right now qualiMap is ok flag to opt in.  add alignment free support for RNAseq analysis e.g salmon/kalisto  is there need for circular RNA support?", 
            "title": "Ideas for future releases"
        }, 
        {
            "location": "/roadmap/#changelog", 
            "text": "", 
            "title": "Changelog"
        }, 
        {
            "location": "/roadmap/#151", 
            "text": "several bug fixes including  #18 , hisat2 related bugs, and samtools sort memory bug  in relation to bug  #18  added extra  sanity check  general code improvement and maintenance  updated docs and included new installation method  improved python scripts, made them user executable and updated docs with how to use them  updated UCSC binaries due to libpng12 issue", 
            "title": "1.5.1"
        }, 
        {
            "location": "/roadmap/#150", 
            "text": "fixed bugs and improved python scripts, also migrated them to python3  fixed issues: #16, #15, #20, #21  fixed bug in handling remote reference files  added  mk_igv_links.py  script as general utility script  added FASTQ trimmer -  skewer  internal code improvements, made code more readable and clean  Re-wrote bam files processing, i.e sorting and marking duplicates. Changed sorting from  picard SortSam  to  samtools sort . Kept  picard MarkDuplicates  for conventional marking duplicates, but add  Je-suite  for UMI based de-duplication.  Made improvements in handling of featureCounts output for multiqc purposes.  completely removed threads and memory parameters, now each task's gets it's own cpu and mem setting all through sik.config. This makes it more cluster friendly as well   Added more metrics gathering; samtools qc metrics:   flagstat  idxstats  stats     Improved many different task's dependencies flow.    changed few command lines options; new options:   -all  -counts  -mdups  -qc  -umi  -trim     removed command line options:   -metrics  -fastqc  -prePro  -threads  -memory     all flags changes have some backward compatibility, i.e will set the closet new option.", 
            "title": "1.5.0"
        }, 
        {
            "location": "/roadmap/#149", 
            "text": "Fixed BWA indexing problem. Problem around STAR and Hisat2 aligners pass index as directory whereas bwa as a file, had to fight to make all different options i.e  -refFiles  and  -genomeIdx  to work  Included proper support for SAF file format handled through -gtfFile flag i.e user can pass GTF, GFF or SAF through that flag and will still get counts  Imporved handling of urls for refrence files  -fastaRef ,  -gtfFile  now works with all bds supported url types. Also fixed tarball url handling through  -fqDir  Improved code readability in several places and included  canFail  flag for making degust reads counts files.  Fixed a bug in  exonicRates  function was passing \"wrong\" gtf file path  Updated docs, added more explanation on how RNAsik ticks. Included a roadmap to allow better time and features management  fixed  -pairIds  bug, courtesy @stu2 (PR #6) and samples sheet file making  Improved python script, including several small bug fixing", 
            "title": "1.4.9"
        }, 
        {
            "location": "/roadmap/#148", 
            "text": "added new feature: coverage plots generation  added new feature: ability to pass previously generated references directory, saves spaces and time  fixed strand guessing script, should be better at guessing now  moved to  mkdocs  for documents compilation and deployment to gh_pages  added new python script to make degust file, this simplifies and strengthens the code  improved readability of the code  improved and fixed bugs in handling fastq files and assignment of fastq to sample names", 
            "title": "1.4.8"
        }, 
        {
            "location": "/roadmap/#147", 
            "text": "fixed STAR memory allocation issue, now user can run with fewer cpus without a worry for STAR spawning multiple tasks causing out of memory issue.  made BASH wrapper (not ideal) for  RNAsik  this is capture  bds  logs including report.html which is rather valuable piece of information about the run  introduced another new aligner -  bwa mem  to do bacterial RNAseq.  introduced multiqc config file in  configs  directory now  completely removed  -fqRegex  and added sanity check for paired end data. If R2 is found and -paired isn't set or vice verse then error message sent  several general bug fixes  simplified help menu", 
            "title": "1.4.7"
        }, 
        {
            "location": "/roadmap/#146", 
            "text": "generalised aligner's call, this will make it easier to add new aligners into RNAsik  included support for  hista2 aligner  added samplesSheep logging  fixed few minor bugs and improved code quality  added  canFail  option to several non crucial tasks, allowing pipeline to continue if some task failed of fastqc and qualimap to allow them to fail as those are non essential tasks.   Tweet to @kizza_a     \nShare", 
            "title": "1.4.6"
        }, 
        {
            "location": "/install/", 
            "text": "Using bio-ansible\n\n\nQuick start\n\n\n\n\nwatch it on youtube\n\n\n\n\nsudo apt-get install unzip make gcc git python-virtualenv\nsudo apt-get install zlib1g-dev libbz2-dev liblzma-dev libncurses5-dev\nsudo apt-get install openjdk-8-jdk ant golang-go\nsudo apt-get install git htop tmux vim\n\nvirtualenv ~/ansible_env\nsource ~/ansible_env/bin/activate\n\npip install --upgrade pip\npip install ansible\n\ngit clone https://github.com/MonashBioinformaticsPlatform/bio-ansible\ncd bio-ansible/\nansible-playbook -i hosts bio.yml --tags dirs,bds,rnasik,star,bwa,hisat2,subread,samtools,htslib,bedtools,picard,qualimap,fastqc,multiqc\n\nexport PATH=$HOME/bioansible/software/apps/BigDataScript-0.99999g:$PATH\nexport PATH=$HOME/bioansible/software/apps/RNAsik-pipe-1.4.9/bin:$PATH\n\nRNAsik\n\n\n\n\nTools prerequisites\n\n\nI tried to account for every sub-dependency in \nbio-ansible\n, definitely checked against vanilla ubuntu 16.04 linux distro, however other systems/linux distros might have slight deviation from this. If you run into trouble please double check dependencies for the tool that is failing.\nThere is quite a spectrum of languages there in the pipeline, C/C++, java and python so far. One can image the difficulty to accommodate every distro and/or system. I'm doing my best !\n\n\n\n\nBigDataScript\n\n\nSTAR aligner\n\n\nsubread\n\n\nsamtools\n\n\nbedtools2\n\n\nPicard tools\n\n\nQualiMap\n\n\nMultiQC\n \n\n\nFastQC\n\n\n\n\nSystem prerequisites\n\n\nIn order to install system dependencies you'll need admin privilege i.e \nsudo\n\n\nGeneral\n, these are you \"stock\" utils, that most running system will/should have\n\n\n\n\nunzip\n\n\nmake\n\n\ngcc\n\n\ngit\n\n\npython-virtualenv\n\n\n\n\nsudo apt-get install unzip make gcc git python-virtualenv\n\n\n\n\nsamtools, htslib and bwa deps\n, these are some what specific libraries\n\n\n\n\nzlib1g-dev\n \n\n\nlibbz2-dev\n \n\n\nliblzma-dev\n\n\nlibncurses5-dev\n\n\n\n\nsudo apt-get install zlib1g-dev libbz2-dev liblzma-dev libncurses5-dev\n\n\n\n\nJava and BigDataScript\n, these again rather generic packages, except golang. \nNote that golang is pretty easy to install, comes as a pre-compiled binary \nhere\n if you don't want to get it through system package mamanger\n\n\n\n\nopenjdk-8-jdk\n \n\n\nant\n\n\ngolang-go\n\n\n\n\nsudo apt-get install openjdk-8-jdk ant golang-go\n\n\n\n\nExtras\n, these are optional dependencies, but \ntmux\n especially recommended as pipeline run could take some time to complete\n\nprovided you are doing this on remote machine (server), which is also recommended\n\n\n\n\nhtop\n\n\ntmux\n\n\nvim\n\n\n\n\nsudo apt-get install git htop tmux vim\n\n\n\n\nRunning bio-ansible\n\n\nFollow \nansible installation guid\n to get ansible then:\n\n\ngit clone https://github.com/MonashBioinformaticsPlatform/bio-ansible\ncd bio-ansible/\nansible-playbook -i hosts bio.yml --tags dirs,bds,rnasik,star,bwa,hisat2,subread,samtools,htslib,bedtools,picard,qualimap,fastqc,multiqc\n\n\n\n\nAlternative installation method for RNAsik\n\n\nIf you have all of the tools installed and you just need \nRNAsik\n you can simply \ngit clone\n it. It doesn't require any\nother installations/compilations. BUT you do need to have \nBigDataScript\n installed\nand have it in your \nPATH\n for \nRNAsik\n to run\n\n\ngit clone https://github.com/MonashBioinformaticsPlatform/RNAsik-pipe\npath/to/RNAsik-pipe/bin/RNAsik\n\n\n\n\nMake RNAsik analysis ready\n\n\nbio-ansible\n is complete bioinformatics stack (with heavily genomics focus at this stage) deployment written in ansible script, which depending on a type of deployment might require admin privilege i.e \nsudo\n.\nGiven that \nsystem prerequisites\n are satisfied one \ndon't\n need \nsudo\n to install bioinformatics stack, primarily \nbio_tools\n.\n\n\nIn this docs there is an assumption that user either has \nsudo\n rights and/or able to install \nsystem prerequisites\n OR already has those dependencies installed and therefore can simply use \nbio-ansible\n as per installing \nRNAsik\n section above to get all required tools dependencies.\n\n\nAlso right now \nbio-ansible\n is focused on a particular tools/enviroment management type, which is \nlmod\n, where one can \nmodule load samtools\n into their environment for use, by default \nsamtools\n isn't available in the current (shell) environment. This is rather common approach on HPC clusters. Because of that type of installation, if user doesn't have pre-installed \nlmod\n they will needs to either \nexport PATH\n for every tool (sounds a bit annoying), OR \nexport\n \nRNAsik\n into your \nPATH\n and them simply let \nRNAsik\n know where tools are through \n-configFile\n option.\n\n\nexport PATH=$HOME/bioansible/software/apps/BigDataScript-0.99999g:$PATH\nexport PATH=$HOME/bioansible/software/apps/RNAsik-pipe-1.4.9/bin:$PATH\n\n\n\n\ncopy these lines into file e.g sik.config and add \n-configFile path/to/sik.config\n to \nRNAsik\n\n\nstarExe = $HOME/bioansible/software/apps/STAR-2.5.2b/STAR\nhisat2Exe = $HOME/bioansible/software/apps/hisat2-2.1.0/bin/hisat2\nbwaExe = $HOME/bioansible/software/apps/bwa-v0.7.15/bwa\nsamtoolsExe = $HOME/bioansible/software/apps/samtools-1.4.1/bin/samtools\nbedtoolsExe = $HOME/bioansible/software/apps/bedtools2-2.25.0/bin/bedtools\ncountsExe = $HOME/bioansible/software/apps/subread-1.5.2/bin/featureCounts\nfastqcExe = $HOME/bioansible/software/apps/fastqc-0.11.5/fastqc\npythonExe = python\npicardExe = java -Xmx6g -jar $HOME/bioansible/software/apps/picard-2.17.10/picard.jar\nqualimapExe = $HOME/bioansible/software/apps/qualimap_v2.2.1/qualimap\nmultiqcExe = $HOME/bioansible/software/apps/multiqc-1.4/bin/multiqc\n\n\n\n\nIf the user happens to have \nlmod\n installed, simply \nmodule use $HOME/bioansible/software/modules/bio\n to let \nlmod\n know about new modules and then simply \nmodule load RNAsik-pipe\n, which will automatically \"pull\" other dependencies into your environment. You can check that by \nmodule list\n to see what is in your environment", 
            "title": "Advance"
        }, 
        {
            "location": "/install/#using-bio-ansible", 
            "text": "", 
            "title": "Using bio-ansible"
        }, 
        {
            "location": "/install/#quick-start", 
            "text": "watch it on youtube   sudo apt-get install unzip make gcc git python-virtualenv\nsudo apt-get install zlib1g-dev libbz2-dev liblzma-dev libncurses5-dev\nsudo apt-get install openjdk-8-jdk ant golang-go\nsudo apt-get install git htop tmux vim\n\nvirtualenv ~/ansible_env\nsource ~/ansible_env/bin/activate\n\npip install --upgrade pip\npip install ansible\n\ngit clone https://github.com/MonashBioinformaticsPlatform/bio-ansible\ncd bio-ansible/\nansible-playbook -i hosts bio.yml --tags dirs,bds,rnasik,star,bwa,hisat2,subread,samtools,htslib,bedtools,picard,qualimap,fastqc,multiqc\n\nexport PATH=$HOME/bioansible/software/apps/BigDataScript-0.99999g:$PATH\nexport PATH=$HOME/bioansible/software/apps/RNAsik-pipe-1.4.9/bin:$PATH\n\nRNAsik", 
            "title": "Quick start"
        }, 
        {
            "location": "/install/#tools-prerequisites", 
            "text": "I tried to account for every sub-dependency in  bio-ansible , definitely checked against vanilla ubuntu 16.04 linux distro, however other systems/linux distros might have slight deviation from this. If you run into trouble please double check dependencies for the tool that is failing.\nThere is quite a spectrum of languages there in the pipeline, C/C++, java and python so far. One can image the difficulty to accommodate every distro and/or system. I'm doing my best !   BigDataScript  STAR aligner  subread  samtools  bedtools2  Picard tools  QualiMap  MultiQC    FastQC", 
            "title": "Tools prerequisites"
        }, 
        {
            "location": "/install/#system-prerequisites", 
            "text": "In order to install system dependencies you'll need admin privilege i.e  sudo  General , these are you \"stock\" utils, that most running system will/should have   unzip  make  gcc  git  python-virtualenv   sudo apt-get install unzip make gcc git python-virtualenv  samtools, htslib and bwa deps , these are some what specific libraries   zlib1g-dev    libbz2-dev    liblzma-dev  libncurses5-dev   sudo apt-get install zlib1g-dev libbz2-dev liblzma-dev libncurses5-dev  Java and BigDataScript , these again rather generic packages, except golang. \nNote that golang is pretty easy to install, comes as a pre-compiled binary  here  if you don't want to get it through system package mamanger   openjdk-8-jdk    ant  golang-go   sudo apt-get install openjdk-8-jdk ant golang-go  Extras , these are optional dependencies, but  tmux  especially recommended as pipeline run could take some time to complete provided you are doing this on remote machine (server), which is also recommended   htop  tmux  vim   sudo apt-get install git htop tmux vim", 
            "title": "System prerequisites"
        }, 
        {
            "location": "/install/#running-bio-ansible", 
            "text": "Follow  ansible installation guid  to get ansible then:  git clone https://github.com/MonashBioinformaticsPlatform/bio-ansible\ncd bio-ansible/\nansible-playbook -i hosts bio.yml --tags dirs,bds,rnasik,star,bwa,hisat2,subread,samtools,htslib,bedtools,picard,qualimap,fastqc,multiqc", 
            "title": "Running bio-ansible"
        }, 
        {
            "location": "/install/#alternative-installation-method-for-rnasik", 
            "text": "If you have all of the tools installed and you just need  RNAsik  you can simply  git clone  it. It doesn't require any\nother installations/compilations. BUT you do need to have  BigDataScript  installed\nand have it in your  PATH  for  RNAsik  to run  git clone https://github.com/MonashBioinformaticsPlatform/RNAsik-pipe\npath/to/RNAsik-pipe/bin/RNAsik", 
            "title": "Alternative installation method for RNAsik"
        }, 
        {
            "location": "/install/#make-rnasik-analysis-ready", 
            "text": "bio-ansible  is complete bioinformatics stack (with heavily genomics focus at this stage) deployment written in ansible script, which depending on a type of deployment might require admin privilege i.e  sudo .\nGiven that  system prerequisites  are satisfied one  don't  need  sudo  to install bioinformatics stack, primarily  bio_tools .  In this docs there is an assumption that user either has  sudo  rights and/or able to install  system prerequisites  OR already has those dependencies installed and therefore can simply use  bio-ansible  as per installing  RNAsik  section above to get all required tools dependencies.  Also right now  bio-ansible  is focused on a particular tools/enviroment management type, which is  lmod , where one can  module load samtools  into their environment for use, by default  samtools  isn't available in the current (shell) environment. This is rather common approach on HPC clusters. Because of that type of installation, if user doesn't have pre-installed  lmod  they will needs to either  export PATH  for every tool (sounds a bit annoying), OR  export   RNAsik  into your  PATH  and them simply let  RNAsik  know where tools are through  -configFile  option.  export PATH=$HOME/bioansible/software/apps/BigDataScript-0.99999g:$PATH\nexport PATH=$HOME/bioansible/software/apps/RNAsik-pipe-1.4.9/bin:$PATH  copy these lines into file e.g sik.config and add  -configFile path/to/sik.config  to  RNAsik  starExe = $HOME/bioansible/software/apps/STAR-2.5.2b/STAR\nhisat2Exe = $HOME/bioansible/software/apps/hisat2-2.1.0/bin/hisat2\nbwaExe = $HOME/bioansible/software/apps/bwa-v0.7.15/bwa\nsamtoolsExe = $HOME/bioansible/software/apps/samtools-1.4.1/bin/samtools\nbedtoolsExe = $HOME/bioansible/software/apps/bedtools2-2.25.0/bin/bedtools\ncountsExe = $HOME/bioansible/software/apps/subread-1.5.2/bin/featureCounts\nfastqcExe = $HOME/bioansible/software/apps/fastqc-0.11.5/fastqc\npythonExe = python\npicardExe = java -Xmx6g -jar $HOME/bioansible/software/apps/picard-2.17.10/picard.jar\nqualimapExe = $HOME/bioansible/software/apps/qualimap_v2.2.1/qualimap\nmultiqcExe = $HOME/bioansible/software/apps/multiqc-1.4/bin/multiqc  If the user happens to have  lmod  installed, simply  module use $HOME/bioansible/software/modules/bio  to let  lmod  know about new modules and then simply  module load RNAsik-pipe , which will automatically \"pull\" other dependencies into your environment. You can check that by  module list  to see what is in your environment", 
            "title": "Make RNAsik analysis ready"
        }
    ]
}